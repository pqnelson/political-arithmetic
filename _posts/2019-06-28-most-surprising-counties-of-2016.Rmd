---
title: "Most Surprising Counties"
author: "Alex Nelson"
date: "6/28/2019"
output: 
  md_document:
    variant: gfm
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE}
library(knitr)
library(kableExtra)
library(LaplacesDemon) # KLD()
library(zoo) # for rollmean()
library(tidyverse)
library(rmarkdown)
library(ggthemes)

library(usmap)

knitr::opts_chunk$set(echo = TRUE)
```
```{r, include=FALSE}
source("../R/states.R")
source("../R/presidential_elections.R")
```


We can measure surprise using the Kullbackâ€“Leibler divergence, niftily provided by the `LaplacesDemon::KLD()` function. This quantifies how surprising the 2016 election results were, given we were assuming a result more closely resembling 2012.

```{r}
county_results <- make_party_into_factor(load_obj(county_path))
election_2004 <- filter(county_results, year == 2004)
election_2008 <- filter(county_results, year == 2008)
election_2012 <- filter(county_results, year == 2012)
election_2016 <- filter(county_results, year == 2016)
```

Now, we treat each county like a three-sided coin, with the proportion of the vote as a proxy for probability.

```{r}
conj_prob <- function(election) {
  prop <- election %>%
    mutate(party = ifelse(party == "democrat", party,
                          ifelse(party == "republican", party,
                                 "$third-party"))) %>%
    group_by(year,state,county,party,FIPS) %>%
    transmute(probability = candidatevotes/totalvotes) %>%
    ungroup() %>%
    unique
  prop$probability[is.na(prop$probability)] <- 0
  prop
}

prop_2004 <- conj_prob(election_2004)
prop_2008 <- conj_prob(election_2008)
prop_2012 <- conj_prob(election_2012)
prop_2016 <- conj_prob(election_2016)

probabilities2008 <- inner_join(select(prop_2004, -year),
                                select(prop_2008, -year),
                                by = c("state", "county", "party"))

probabilities2012 <- inner_join(select(prop_2008, -year),
                                select(prop_2012, -year),
                                by = c("state", "county", "party"))

probabilities2016 <- inner_join(select(prop_2012, -year),
                                select(prop_2016, -year),
                                by = c("state", "county", "party"))
```

We will now try to assign to each county the measure of surprise.

```{r}
compute_surprise <- function(data) {
  KLD(data$probability.x, data$probability.y, base=2)$sum.KLD.py.px
}

election_surprise <- function(probs) {
  probs %>%
    filter(party != "NA") %>%
    group_by(state,county) %>%
    nest() %>%
    mutate(surprise = map(data, compute_surprise)) %>%
    unnest(surprise, .drop = TRUE) %>%
    arrange(-surprise)
}

results08 <- election_surprise(probabilities2008)
results12 <- election_surprise(probabilities2012)
results16 <- election_surprise(probabilities2016)
```

## Statistically Significant Surprises

Note that Blair County, PA has a surprise of 0.01; did it vote differently than 2012 with any statistical significance (with alpha = 0.05, say):

```{r}
blair_pa_2012 <- election_2012 %>%
  filter(state == "Pennsylvania", county == "Blair", party == "republican")  %>%
  transmute(p = candidatevotes/totalvotes)

blair_pa_2016 <- election_2016 %>%
  filter(state == "Pennsylvania", county == "Blair", party == "republican")  %>%
  transmute(x = candidatevotes,
            n = totalvotes)

prop.test(blair_pa_2016$x, blair_pa_2016$n, blair_pa_2012$p)
```
The `p-value < 2.2e-16`, which tells us, yes, Blair county behaved differently in 2016 with statistical significance.

We could try generalizing this. Going county-by-county will be no good. We will instead work with state-level results, then examine each state for statistically different voting patterns among specifically Republicans.

```{r}
state_results <- make_party_into_factor(load_obj(state_path))
election_2012 <- filter(state_results, year == 2012)
election_2016 <- filter(state_results, year == 2016)
prop_2012 <- election_2012 %>%
  group_by(year,state,party) %>%
  transmute(probability = candidatevotes/totalvotes) %>%
  ungroup()
# prop_2012$probability[is.na(prop_2012$probability)] <- 0
```

We are interested in the situation where Trump did better than Republicans historically perform, as compared to the past race in 2012. This can be approximated using the p-value from the Binomial test.

```{r}
data <- inner_join(filter(election_2016, party == "republican", writein == F),
                   filter(prop_2012, party == "republican"),
                   by = c("state","party")) %>%
  filter(!is.na(totalvotes), is.state(state)) %>%
  select(state,candidatevotes,totalvotes,probability) %>%
  group_by(state) %>%
  mutate(trump_2016 = candidatevotes/totalvotes,
         p.value = binom.test(x = candidatevotes, n = totalvotes, p = probability, alternative = "greater")$p.value) %>%
  ungroup() %>%
  rename(republican_2012 = probability) %>%
  arrange(p.value)

kable(data)
```

There are onl 28 states where this p-value is smaller than units (of which 24 are really the interesting states). So we've narrowed down our attention to 24 states where Trump over-performed (compared to 2012). Really, there are only about a half dozen states worth investigating since they were won by Trump:

```{r}
kable(filter(data, trump_2016 > republican_2012, trump_2016 > 0.45))
```

## Picking Out the Relevant States

We can pick out the relevant states, but the criteria we have is exceedingly generous. We end up with 25 states, with genuine surprises like Michigan, but with false-positives like Alabama.

```{r}
relevant_states <- data %>%
  filter(p.value < 0.1) %>% 
  select(state) %>%
  `[[`("state") %>%
  sort
```

We could then compute the surprise for each of these states:

```{r}
relevant_surprise <- results16 %>%
  filter(state %in% relevant_states) %>%
  group_by(state) %>%
  summarize(total_surprise = sum(surprise)) %>%
  arrange(-total_surprise)
relevant_surprise
```

Now we can filter out the states less surprising than Alabama:

```{r}
relevant_surprise2 <- filter(relevant_surprise,
                            total_surprise > relevant_surprise[which(relevant_surprise$state=="Alabama"),]$total_surprise) %>%
  arrange(-total_surprise)
```

This eliminates 10 false-positives. Lets try also weighing these states by electoral delegates:

```{r}
relevant_surprise2 %>%
  group_by(state) %>%
  mutate(ec = electoral_delegates[[state]],
         weighted_surprise = ec*total_surprise) %>%
  arrange(-weighted_surprise)
```
We shouldn't be surprised Nebraska was won by Trump, it's a red state... as are all the others with a small `weighted_surprise` value. So we can drop these as false-positives as well:

```{r}
true_surprises <- relevant_surprise2 %>%
  group_by(state) %>%
  mutate(electoral_delegates = electoral_delegates[[state]],
         weighted_surprise = electoral_delegates*total_surprise) %>%
  arrange(-weighted_surprise) %>%
  filter(weighted_surprise > 10)
```

The true surprises, where Trump overperformed, appears to be:

```{r}
kable(true_surprises)
```

## Surprising Counties

Lets examine the counties in these genuinely surprising states.

```{r}
surprising_counties <- results16 %>%
  filter(state %in% true_surprises$state) %>%
  arrange(-surprise)
surprising_counties$state <- as.factor(surprising_counties$state)
```

Lets draw a box-plot for the data.

```{r surprise-boxplots}
ggplot(filter(surprising_counties, state!="Nebraska", state!="North Dakota"), aes(x=fct_reorder(state, -surprise), y=surprise)) +
  geom_boxplot() +
  theme_fivethirtyeight(base_size = 8)
```



```{r random-rustbelt-states}
ggplot(filter(surprising_counties, state %in% c("Iowa", "Ohio", "Michigan", "Kentucky", "Wisconsin")), aes(x = surprise, fill = state)) +
  geom_histogram() +
  geom_density(alpha=.2, fill="#FF6666") +
  theme_fivethirtyeight(base_size = 8)
```


```{r non-random-rustbelt-states}
ggplot(filter(surprising_counties, !(state %in% c("Iowa", "Ohio", "Michigan", "Kentucky", "Wisconsin"))), aes(x = surprise, fill = state)) +
  geom_histogram() +
  geom_density(alpha=.2, fill="#FF6666") +
  theme_fivethirtyeight(base_size = 8)
```

This looks like a log-normal distribution, lets see what happens if we exponentiate the surprise:

```{r log-normal-guess}
ggplot(surprising_counties %>%
         filter(surprise > exp(-6)) %>%
         mutate(exp_surprise = log(surprise)), aes(x = exp_surprise, fill = state)) +
  geom_histogram() +
  geom_density(alpha=.2, fill="#FF6666") +
  theme_fivethirtyeight(base_size = 8)
```

```{r}
Mode <- function(x, na.rm = FALSE) {
  if (na.rm) {
    x = x[!is.na(x)]
  }

  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

surprising_counties %>%
  group_by(state) %>%
  summarize(mode = Mode(surprise, na.rm = T),
            avg = mean(surprise),
            median = median(surprise),
            sd = sd(surprise),
            sum = sum(surprise)) %>%
  arrange(-mode)
```
Considering we are effectively looking at a 3-sided die (which would have for a given state's results), which has its entropy be maximized for a "fair die" with entropy `r log(3, base=2)`. There are 17 such states, but only 12 were really surprising:

```{r}
results16 %>%
  group_by(state) %>%
  summarize(sum = sum(surprise)) %>%
  filter(sum > log(3, base = 2)) %>%
  arrange(-sum)
```
In biological systems, the "sum" column would measure how far we are from equilibrium (c.f., [arXiv:1512.02742](https://arxiv.org/abs/1512.02742)). Intuitively, it's a measure of "surprise" --- how far off we'd be supposing 2016 would unfold like 2012. These states in particular are where we were just dead wrong in our suppositions. 

```{r}
surprise_for_state <- function(p_states, y) {
  inner_join(filter(p_states, year == y - 4),
             filter(p_states, year == y),
             by = c("state", "party")) %>%
    group_by(state) %>%
    nest() %>%
    mutate(surprise = map(data, compute_surprise)) %>%
    unnest(surprise, .drop = TRUE) %>%
    arrange(-surprise)
}
```


```{r}
prop_states <- state_results %>%
  mutate(party = ifelse(party == "democrat", "democrat",
                        ifelse(party == "republican", "republican",
                               "$third-party"))) %>%
  group_by(year,state,party) %>%
  transmute(probability = sum(candidatevotes)/totalvotes) %>%
  ungroup() %>%
  unique
prop_states$party <- as.factor(prop_states$party)

surprise_1988 <- surprise_for_state(prop_states, 1988)
surprise_1992 <- surprise_for_state(prop_states, 1992)
surprise_1996 <- surprise_for_state(prop_states, 1996)
surprise_2000 <- surprise_for_state(prop_states, 2000)
surprise_2004 <- surprise_for_state(prop_states, 2004)
surprise_2008 <- surprise_for_state(prop_states, 2008)
surprise_2012 <- surprise_for_state(prop_states, 2012)
surprise_2016 <- surprise_for_state(prop_states, 2016)
```

How much surprise was there, overall, compared to 2012? We plot a histogram of the states's "surprise value" by year:

```{r historic-surprises}
surprise_1988$year <- 1988
surprise_1992$year <- 1992
surprise_1996$year <- 1996
surprise_2000$year <- 2000
surprise_2004$year <- 2004
surprise_2008$year <- 2008
surprise_2016$year <- 2016
surprise_2012$year <- 2012
surprise_elections <- rbind(surprise_1988, rbind(surprise_1992, rbind(surprise_1996, rbind(surprise_2000, rbind(surprise_2004, rbind(surprise_2008, rbind(surprise_2012, surprise_2016)))))))
surprise_elections$year <- as.factor(surprise_elections$year)

ggplot(surprise_elections, aes(x=surprise, fill=year)) +
  geom_histogram() +
  theme_fivethirtyeight(base_size = 8)
```

Why is the 1992 election so surprising? Well, Ross Perot did **surprisingly** well in the popular vote, receiving about 20% of the popular vote. He _literally_ did **surprisingly** well.

```{r}
surprise_elections %>%
  group_by(year) %>%
  summarize(surprise = sum(surprise))
```



## Overperformed...compared to what?

We should take the time here to note we should be a little careful upon further analysis to take, e.g., the running mean of the percentages for Republican candidates rather than just the last election's results. Well, we would take the running average for the Republican candidate, the Democratic candidate, and "Third Party candidate", then renormalize to make certain the weighted means are transformed into proportions.

```{r}
```

```{r}
prop_states %>%
  arrange(state,party,year) %>%
  group_by(state,party) %>%
  summarize(moving_prob = last(rollmean(probability, k = 4))) %>%
  group_by(state) %>%
  mutate(moving_prob = moving_prob/sum(moving_prob)) %>%
  ungroup()
```

Now we can answer the question more directly, how well did Trump do compared to an "average Republican"?

```{r}
priors_2016 <- prop_states %>%
  arrange(state,party,year) %>%
  group_by(state,party) %>%
  summarize(moving_prob = last(rollmean(probability, k = 4))) %>%
  ungroup() %>%
  mutate(year = 2012) %>%
  group_by(state) %>%
  mutate(probability = moving_prob/sum(moving_prob)) %>%
  ungroup()

comp_2016 <- inner_join(priors_2016,
                        filter(prop_states, year==2016),
                        by=c("state", "party")) %>%
  group_by(state) %>%
  nest() %>%
  mutate(surprise = map(data, compute_surprise)) %>%
  unnest(surprise, .drop = TRUE) %>%
  arrange(-surprise) %>%
  mutate(year = 2016)
```

Lets see everything more surprising than Alabama:

```{r}
kable(select(filter(comp_2016, surprise > comp_2016[which(comp_2016$state=="Alabama"),]$surprise), -year))
```

The low surprise for Michigan and Ohio suggests that...we shouldn't be surprised by the outcome. The counterfactual situation where we consider a "replacement candidate" in both parties may be worth plotting, against the real competitors's results.

```{r counterfactual}
surprise_2016$counterfactual <- F
comp_2016$counterfactual <- T
conterfact_2016 <- select(rbind(surprise_2016, comp_2016), -year)

ggplot(conterfact_2016, aes(x=surprise, fill=counterfactual)) +
  geom_histogram(binwidth = 0.005) +
  theme_fivethirtyeight(base_size = 8)
```


Did Trump overperform or Clinton underperform? Or did third party candidates surge unexpectedly? The histogram can't tell, because that's not being measured. All that's measured is the "surprise distribution" across the states, comparing "replacement candidates" to the actual ones. The actual candidates produced more surprising results.

We can sum the differences between _expected_ vote proportions and _actual_ vote proportions.

```{r}
diffs_2016 <- inner_join(select(filter(prop_states, year==2016), -year), select(priors_2016, -year), by=c("state", "party")) %>%
  group_by(state,party) %>%
  transmute(delta = probability.x - probability.y) %>%
  ungroup()
```

Now we should weigh this by the electoral delegates in each state (honestly, it should be the difference in voters in each state, but this is approximated by the electoral delegate count)

```{r}
kable(diffs_2016 %>%
        group_by(state,party) %>%
        mutate(expected_ed = delta*electoral_delegates[[first(state)]]) %>%
        ungroup() %>%
        group_by(party) %>%
        summarize(actual_minus_expected = sum(expected_ed)))
```

So, yes, the third party candidates did better than expected. We could even plot the expected outcome:

```{r expected-map}
plot_usmap(data = priors_2016 %>%
             select(-moving_prob) %>%
             spread(party, probability) %>%
             mutate(winning_margin = republican - democrat - `$third-party`,
                    winning_party = ifelse(winning_margin < 0, -1, 1)),
           values = "winning_party") + 
  scale_fill_continuous(
    low = "blue", high = "red", name = "Margin of Victory"
  ) + theme(legend.position = "right")
```

This would have produced the following results:

```{r}
priors_2016 %>%
  select(-moving_prob) %>%
  spread(party, probability) %>%
  mutate(winning_margin = republican - democrat - `$third-party`,
         winning_party = ifelse(winning_margin < 0, -1, 1)) %>%
  ungroup() %>%
  group_by(state) %>%
  mutate(dem_ed = electoral_delegates[[first(state)]]*ifelse(winning_party==-1, 1, 0),
         rep_ed = electoral_delegates[[first(state)]]*ifelse(winning_party==1, 1, 0)) %>%
  ungroup() %>%
  summarize(democratic_electoral_delegates = sum(dem_ed),
            republican_electoral_delegates = sum(rep_ed)
            )
```



# Third Party Performance: 2016 vs 2012

It just so happens Jill Stein ran in 2012 on the Green party ticket, and Gary Johnson ran in 2012 on the Libertarian ticket. We can compare their respective performances to see if either one improved noticeably. What's the game plan? Well, we should measure how surprising the results were in 2016 compared to 2012, as usual. We partition the election results into "Voted for Johnson" and "Voted for Someone Else" (we repeat this analysis for "Stein" instead of Johnson later), and measure how surprising the result is.

## Johnson's Performance

We first will look at how well Johnson did in each state, for both elections. Unfortunately, our data source's information at the county level is insufficient. So we will be forced to work at the state level.

```{r}
johnson_2012 <- filter(state_results, year == 2012, candidate == "Johnson, Gary")
johnson_2016 <- filter(state_results, year == 2016, candidate == "Johnson, Gary")
```

Lets see...in 2012, Johnson ran in 46 states, but in 2016 he ran in all 50 states. Lets see what states were missing in 2012.

```{r}
johnson_2016 %>%
  filter(!(state %in% johnson_2012$state)) %>%
  select(state) %>%
  unique
```

Some of these states (Michigan, Wisconsin) are critical to the 2016 election outcome.

```{r}
johnson_2012 %>%
  group_by(state) %>%
  transmute(p = sum(candidatevotes)/totalvotes,
            p_other = 1 - p) %>%
  ungroup()
```

We can now examine the (electoral delegate weighted) surprise for each state, to see how well Johnson performed compared to our 2012 expectations. The weighted surprise is:

```{r}
johnson_surprise <- inner_join(johnson_2012 %>%
             group_by(state) %>%
             transmute(p = sum(candidatevotes)/totalvotes,
                       p_other = 1 - p) %>%
             ungroup(),
           johnson_2016 %>%
             group_by(state) %>%
             transmute(p = sum(candidatevotes)/totalvotes,
                       p_other = 1 - p) %>%
             ungroup(),
           by = "state") %>%
  group_by(state) %>%
  transmute(surprise = KLD(px = c(p.x, p_other.x), py = c(p.y, p_other.y), base = 2)$sum.KLD.py.px*electoral_delegates[[first(state)]]) %>%
  arrange(-surprise)

johnson_surprise
```

Johnson did surprisingly well compared to his 2012 performance. At this rate, he could be president in a century.

```{r johnson-scatterplot}
johnson_surprise$state <- factor(johnson_surprise$state, levels=states_and_dc())

ggplot(johnson_surprise, aes(x=fct_reorder(state, surprise), y=surprise)) +
  geom_point() +
  coord_flip() +
  theme_fivethirtyeight(base_size = 8)
```


## Stein's Performance

We can do likewise for Stein, finding the state-level data.

```{r}
Stein_2012 <- filter(state_results, year == 2012, candidate == "Stein, Jill")
Stein_2016 <- filter(state_results, year == 2016, candidate == "Stein, Jill")
```


We find the (electoral delegate weighted) surprise for Stein:

```{r}
stein_surprise <- inner_join(Stein_2012 %>%
             group_by(state) %>%
             transmute(p = sum(candidatevotes)/totalvotes,
                       p_other = 1 - p) %>%
             ungroup(),
           Stein_2016 %>%
             group_by(state) %>%
             transmute(p = sum(candidatevotes)/totalvotes,
                       p_other = 1 - p) %>%
             ungroup(),
           by = "state") %>%
  group_by(state) %>%
  transmute(surprise = KLD(px = c(p.x, p_other.x), py = c(p.y, p_other.y), base = 2)$sum.KLD.py.px*electoral_delegates[[first(state)]]) %>%
  arrange(-surprise)

stein_surprise
```

This is seemingly smaller than Johnson's improvement. But that's misleading, because of the use of scientific notation. If we plot out Stein's performance, things clear up quickly:

```{r stein-scatterplot}
stein_surprise$state <- factor(stein_surprise$state, levels=states_and_dc())

ggplot(stein_surprise, aes(x=fct_reorder(state, surprise), y=surprise)) +
  geom_point() +
  coord_flip() +
  theme_fivethirtyeight(base_size = 8)
```

Lets try to plot these together.

```{r joint-scatterplot}
stein_surprise$candidate <- "stein"
johnson_surprise$candidate <- "johnson"

third_party_surprise <- rbind(stein_surprise,
                              johnson_surprise)
# third_party_surprise$candidate <- factor(third_party_surprise$candidate, levels=states)

ggplot(third_party_surprise, aes(x=fct_reorder(state, surprise), y=surprise, shape=candidate, color=candidate)) +
  geom_point() +
  coord_flip() +
  theme_fivethirtyeight(base_size = 8)
```

Just curious, what was the average ratio of Johnson's improvement to Stein's improvement?

```{r}
efficiency <- inner_join(stein_surprise, johnson_surprise, by="state") %>% group_by(state) %>% transmute(rat = surprise.y/surprise.x) %>% arrange(-rat)

mean(efficiency$rat)
```

**Observe:** Johnson's improvement from 2012 to 2016 was `r mean(efficiency$rat)` times that of Stein's improvement (an order of magnitude better!).

Lets restrict focus to states we're really interested in, the surprising ones.

```{r joint-swing-scatterplot}
swing_states <- c("Florida", "Michigan", "Pennsylvania", "Wisconsin", "Iowa", "Indiana", "North Carolina", "Ohio", "Nebraska")
ggplot(filter(third_party_surprise, state %in% swing_states), 
       aes(x=fct_reorder(state, -surprise), 
           y=surprise, 
           shape=candidate, 
           color=candidate)) +
  geom_point() +
  theme_fivethirtyeight(base_size = 8)
```

With the exception of Flordia, Jill Stein really didn't improve that much compared to Johnson.

```{r}
johnson_2012 %>%
  filter(state %in% swing_states) %>%
  select(state, candidatevotes, totalvotes) %>%
  group_by(state) %>%
  mutate(p = candidatevotes/totalvotes) %>%
  ungroup()
```

```{r}
johnson_2016 %>%
  filter(state %in% swing_states) %>%
  select(state, candidatevotes, totalvotes) %>%
  group_by(state) %>%
  mutate(p = candidatevotes/totalvotes) %>%
  ungroup()
```

```{r}
right_join(johnson_2012 %>%
             filter(state %in% swing_states) %>%
             select(state, candidatevotes, totalvotes) %>%
             group_by(state) %>%
             transmute(p_2012 = candidatevotes/totalvotes) %>%
             ungroup(),
           johnson_2016 %>%
             filter(state %in% swing_states) %>%
             select(state, candidatevotes, totalvotes) %>%
             group_by(state) %>%
             transmute(p_2016 = candidatevotes/totalvotes) %>%
             ungroup(),
           by = "state") %>%
  group_by(state) %>%
  mutate(improvement = ifelse(is.na(p_2012), NA, p_2016/p_2012)) %>%
  ungroup()
```



