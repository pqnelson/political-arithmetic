---
title: "How did Trump win 2016?"
author: "Alex Nelson"
date: "6/1/2019"
output:
  md_document:
    variant: gfm
    toc: true
    toc_depth: 6
---

```{r setup, include=FALSE}
library(knitr)
library(kableExtra)

library(dplyr)
library(car) # for vif()

library(assertthat)

library(lsei)
library(mgcv)
library(arm) # for glmer

library(rjags)
library(runjags)
library(coda)
library(HDInterval)

library(tidycensus)
library(tidyverse)
library(ggplot2)
library(ggmcmc)
library(scales) # for muted()
library(rmarkdown)

knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# local code
source("../R/states.R")
source("../R/census.R")
```

# How did Trump Win the White House?

Although we just found, by accident, one way to explain how Trump won the White House (namely, Obama voters from 2012 who switched to vote for Trump in 2016 explain how a sufficient number of swing states...swung...), are there other ways to explain _how Trump won the White House?_

We will use a post-stratified multilevel model training a logistic regression to match against exit poll data, then using those coefficients in a linear regression to try to match the voting results. This is a partially pooled model, so although the coefficients vary state-to-state, they are still "in the same ballpark". For reviews of this approach, see:

- the MRP Primer http://www.princeton.edu/~jkastell/MRP_primer/mrp_primer.pdf
- David Park, Andrew Gelman, Joseph Bafumi, [Bayesian Multilevel Estimation with Poststratification: State-Level Estimates from National Polls](http://www.stat.columbia.edu/~gelman/research/published/parkgelmanbafumi.pdf)
- Rob Trangucci, Imad Ali, Andrew Gelman, Doug Rivers, "Voting patterns in 2016: Exploration using multilevel regression and poststratification (MRP) on pre-election polls" [arXiv:1802.00842](https://arxiv.org/abs/1802.00842)


# Testing Gelman's Model

In [arXiv:1802.00842](https://arxiv.org/abs/1802.00842), Gelman et al. suggest a likely voter model specifically via a logistic regression defined by

```
logit(voted) ~  1 + female + pre_election_poll_for_state +
                (1 | state) + (1 | age) +
                (1 | education) + (1 + pre_election_poll_for_state | ethnicity) +
                (1 | married) + (1 | married:age) +
                (1 | married:state) + (1 | married:ethnicity) +
                (1 | married:gender) + (1 | married:education) +
                (1 | state:gender) + (1 | age:gender) +
                (1 | education:gender) + (1 | ethnicity:gender) +
                (1 | state:ethnicity) + (1 | state:age) +
                (1 | state:education) + (1 | ethnicity:age) +
                (1 | ethnicity:education) + (1 | age:education) +
                (1 | state:education:age) + (1 | education:age:gender)
```

Independently, given someone voted, Gelman and friends setup a logistic regression describing the probability someone voted for Clinton (or, if you prefer, Trump):

```
logit(Trump|voted) ~  1 + female + pre_election_poll_for_state +
                     (1 | state) + (1 | age) +
                     (1 | education) + (1 + pre_election_poll_for_state | ethnicity) +
                     (1 | married) + (1 | married:age) +
                     (1 | married:state) + (1 | married:ethnicity) +
                     (1 | married:gender) + (1 | married:education) +
                     (1 | state:gender) + (1 | age:gender) +
                     (1 | education:gender) + (1 | ethnicity:gender) +
                     (1 | state:ethnicity) + (1 | state:age) +
                     (1 | state:education) + (1 | ethnicity:age) +
                     (1 | ethnicity:education) + (1 | age:education) +
                     (1 | state:education:age) + (1 | education:age:gender)
```

The probability that someone voted for Trump may be computed as `Pr(Trump|voted)Pr(voted)`, using the logistic regressions we just assembled.

## Strategy

We can approximate the coefficients for the logistic regression using exit poll data. Admittedly this is rather _ad hoc_, but we can estimate the margin of error for the various coefficients using the Wilson confidence interval as a proxy for the standard deviation of a Gaussian variable centered at the polled percentages.

This will allow us to test certain hypotheses ("young people don't vote", "there were more older white people than expected", etc.) without having to resort to other data sources, we hope.

## Data Caveat: Exit Polls noisier than CPS estimates

We are using exit polls from CNN and friends, whereas Gelman _et al._ are using CPS estimates, which are much cleaner and much more accurate. Therefore, we should not expect to perfectly reproduce their findings. But it is a starting point for computation.

## Data Caveat 2: Exit Polls Limited in Questions

We cannot approximate every coefficient in the logistic regression from Gelman and friends. We have to restrict it to:

```
logit(voted) ~  1 + female +
                (1 | state) + 
                (1 | age) + (1 | state:age) +
                (1 | state:gender) + (1 | age:gender) +
                (1 | married) + (1 | married:age) + (1 | married:state) + (1 | married:gender) + 
                (1 | state:ethnicity) + (1 | ethnicity:gender) + (1 | ethnicity:age) +
                (1 | education) + (1 | state:education)
```

## Computational Details

So, for the logistic regression coefficients, we will compute the coefficients using a strategy reflected in the following examples for the regression on the probability someone will vote:

+ `female = log(((percent women in national exit poll)*(votes cast nationwide)/(exit poll size))/((number of women nationwide eligible to vote) - ((percent women in national exit poll)*(votes cast nationwide)/(exit poll size)))`
+ `(1|state) = log((votes cast in state)/(number of eligible voters who didn't turn out))`
+ `(1|age) = log((exit poll percentage for age)*(votes cast nationwide)/(sample size of exit poll)/((number of people in age bracket nationwide) - (exit poll percentage for age)*(votes cast nationwide)/(sample size of exit poll)))`
+ `(1 | state:age)` computed like `(1 | age)` but working with state specific exit polls and state specific population data
+ `pre_election_poll_for_state` is the source of the greatest variability, since there are various possible choices for a pre-election prior (e.g., we could use some estimates from 538 or Inside Elections or some other source, or we could use an actual poll).

The center of the Wilson confidence interval is "close enough" to the exit poll percentages that we might as well use the exit poll percentages. The usefulness of the Wilson interval is its width will give us an estimate of a `sigma`, and we can transform the exit poll percentage into the mean of a normally distributed random variable with an estimated standard deviation `sigma`. The only caveat is, since exit polls are noisy, we use `z=4` when computing the sigma (or `z=2` for half the sigma).

We then test the hypotheses using this jury-rigged statistical model. There is no shortage of [explanations](https://fivethirtyeight.com/features/the-real-story-of-2016/) for what happened, which we can examine.

# Loading Data

## Official Results

We load the official results.

```{r}
load('../data/elections/presidential/county/countypres_2000-2016.RData')
states_factors <- factor(append(states(),c("District of Columbia", "nation")))
x$state <- factor(x$state, levels(states_factors))
election_2016 <- x[which(x$year == 2016),]
election_2016$totalvotes[is.na(election_2016$totalvotes)] <- 0
```

## Exit Poll Data

We load CNN's exit poll data. Fortunately, it is stored as a CSV file.

```{r}
exit_poll_path <- "../data/elections/presidential/exit_polls/cnn_04022017.csv"
exit_poll_df <- read.csv(file=exit_poll_path, header=TRUE, sep=",", stringsAsFactors= FALSE)
exit_poll_df$state <- factor(exit_poll_df$state, levels(states_factors))
```

We combine race and gender into a single variable with 7 categories (the 3 race categories [white, black, hispanic] and 2 genders [men, women] combinations, and 1 `other` category). Asians constitute about 4% of the respondents, and "others" constitutes 3%. Whites constitute 71% of respondents, non-whites constitute 29%.

The age brackets are: 18-29, 30-44, 45-64, and 65+. So, 5 categories.

The education brackets are: "High school or less", "Some college", "College graduate", "Postgraduate".

The income brackets are `30k-50k`, `50k-100k`, `100k-150k`, `150k-200k`, `200k-250k`, and `250k+`. We can use the `B19037` table from the ACS5 census data to get the estimates of number of householder (i.e., occupant who either owns or rents the residency as determined by whose name is on the deed, renal agreement, etc.) by age, income, and race. The ACS5 data has finer categories of income brackets. Unfortunately, the age-brackets for this are younger than 25, 25-64, 45-64, 65+, so we need to be careful not to toss out all the census data too quickly.

### Wilson Interval

Before moving on, we will require usage of Wilson confidence intervals to estimate the noisiness of exit poll data.

```{r}
wilson_center <- function(p, n, z) {
  assert_that(all(is.numeric(p)))
  assert_that(all(0 <= p && p <= 1))
  assert_that(all(is.numeric(n)))
  assert_that(all(n > 0))
  assert_that(all(is.numeric(z)))
  assert_that(all(z > 0))
  (p + 0.5*z*z/n)/(1.0 + z*z*1.0/n);
}

wilson_width <- function(p, n, z) {
  assert_that(all(is.numeric(p)))
  assert_that(all(0 <= p && p <= 1))
  assert_that(all(is.numeric(n)))
  assert_that(all(n > 0))
  assert_that(all(is.numeric(z)))
  assert_that(all(z > 0))
  (z/(1.0 + z*z*1.0/n))*sqrt(p*(1.0-p)/n  + (z*0.5/n)**2);
}
```

## Census Data

We load the census data for each state appearing in the exit poll.

```{r}
state_census_data <- function() {
  df <- data.frame()
  for (state in unique(exit_poll_df$state)) {
    if (state != "nation") {
      state_data <- read.csv(file=paste0('../data/census/acs5/2016/', state, '.csv'), header=TRUE, sep=',')
      state_data$state = state;
      df <- rbind(df, state_data)
    }
  }
  df
}
```

We load the census data and renormalize the states, to match the exit poll's `state` factors.

```{r}
census_data <- categorize(state_census_data())
census_data$state <- factor(census_data$state, levels(states_factors))
```

# Estimating Voter Turnout

The constant for logistic regressions is the log odds of the desired result. For us, that's the number of people who turned out to vote, over the number of people who abstained. Using the [election project's estimates](http://www.electproject.org/2016g):

```{r}
votes_for_highest_office <- 136700729.0
eligible_voters <- 230931921.0
beta_0 <- log(votes_for_highest_office/(eligible_voters - votes_for_highest_office))
```

## State Effects

We will compute the coefficient `(1 | state)`. This is `log(Pr(turnout|state)/Pr(!turnout|state)) - log(Pr(turnout)/Pr(!turnout))`.

```{r}
turnout_state_log_coefs <- function(election_data, census_data) {
  census_data %>% 
    group_by(state) %>%
    summarize(vap = sum(voting_age)) %>%
    inner_join(election_data %>% group_by(state) %>% summarize(tv = sum(totalvotes)), by="state") %>%
    transmute(state,
              proportion = tv/vap) %>%
    transmute(state,
              beta_state = log(proportion/(1 - proportion)) - beta_0) %>%
    unique
}
```

```{r}
kable(turnout_state_log_coefs(filter(election_2016,party=="democrat"), census_data))
```



## Ethnicity

Computing `(1 | state:ethnicity)` requires doing this piece-wise, by each state.


```{r}
ethnicity_by_state <- function(state_iter, exit_poll_by_state, election_data, census_data, z=2) {
  lookup <- list("White" = c("white_male", "white_female"),
                 "Black" = c("black_male", "black_female"),
                 "Latino" = c("hispanic_male", "hispanic_female"),
                 "Asian" = c("asian_male", "asian_female"),
                 "Other race" = c("others"))
  
  id <- min(filter(exit_poll_by_state,questions=="Race")$questions_id)
  state_effect <<- turnout_state_log_coefs(filter(election_data, state==state_iter), 
                                           filter(census_data, state == state_iter))

  exit_poll_by_state %>%
    filter(questions_id == id) %>%
    group_by(options) %>%
    transmute(k = lookup[options],
              estimates = (0.01*options_perc - wilson_width(0.01*options_perc, num_respondents, z))*sum(election_data[which(election_data$state %in% state_iter),]$totalvotes),
              pop = sum(census_data[which(census_data$state %in% state_iter), unlist(k)]),
              proportion = min(estimates/pop, 0.99)) %>% 
    transmute(state = state_iter,
              beta_ethnicity_by_state = log(proportion/(1 - proportion)) - state_effect[which(state_effect$state %in% state_iter),]$beta_state) %>%
    as.data.frame()
}
```

Alas, I could not get a slicker way to combine the results together to work, at least not as expected.

```{r}
fun <- function(exit_poll, election_data, census_data, z=2) {
  results = c()
  for (state_iter in unique(exit_poll$state)) {
    if (state_iter != "nation") {
      state_data <- ethnicity_by_state(state_iter, filter(exit_poll, state==state_iter), election_data, census_data, z)
      results <- rbind(results,state_data)
    }
  }
  results
}
```

### Ethnicity and Gender

We can compute`(1 | ethnicity:gender)`

```{r}
turnout_ethnicity_gender_log_coefs <- function(exit_poll, election_data, census_data, z=2) {
  lookup <- list(`White men` = "white_male",
                 `White women` = "white_female",
                 `Black men` = "black_male",
                 `Black women` = "black_female",
                 `Latino men` = "hispanic_male",
                 `Latino women` = "hispanic_female",
                 `Others` = "others")
  exit_poll %>%
    filter(questions=="Race and gender", state=="nation") %>%
    transmute(options,
              k = unlist(lookup[options]),
              estimated_voters = options_perc*0.01*sum(election_data$totalvotes),
              proportion = estimated_voters/sum(census_data[k])
              ) %>%
    transmute(options,
              beta_ethnicity_gender = log(proportion/(1 - proportion)) - beta_0)
}
```

```{r}
kable(turnout_ethnicity_gender_log_coefs(exit_poll_df, election_2016, census_data))
```


### Ethnicity and Age

We compute `(1 | ethnicity:age)`

```{r}
turnout_ethnicity_age_log_coefs <- function(exit_poll, election_data, census_data, z=2) {
  lookup <- list("Whites 18-29" = c("white_male_18_29", "white_female_18_29"),
                 "Whites 30-44" = c("white_male_30_44", "white_female_30_44"),
                 "Whites 45-64" = c("white_male_45_64", "white_female_45_64"),
                 "Whites 65 and older" = c("white_male_retirees", "white_female_retirees"),
                 "Blacks 18-29" = c("black_male_18_29", "black_female_18_29"),
                 "Blacks 30-44" = c("black_male_30_44", "black_female_30_44"),
                 "Blacks 45-64" = c("black_male_45_64", "black_female_45_64"),
                 "Blacks 65 and older" = c("black_male_retirees", "black_female_retirees"),
                 "Latinos 18-29" = c("hispanic_male_18_29", "hispanic_female_18_29"),
                 "Latinos 30-44" = c("hispanic_male_30_44", "hispanic_female_30_44"),
                 "Latinos 45-64" = c("hispanic_male_45_64", "hispanic_female_45_64"),
                 "Latinos 65 and older" = c("hispanic_male_retirees", "hispanic_female_retirees"),
                 "All others" = c("others")
                 )
  exit_poll %>%
    filter(questions=="Age by race", state=="nation") %>%
    transmute(options,
              k = lookup[options],
              estimated_voters = options_perc*0.01*sum(election_data$totalvotes),
              proportion = estimated_voters/sum(census_data[unlist(k)])
    ) %>%
    transmute(options,
              beta_ethnicity_age = log(proportion/(1 - proportion)) - beta_0)
}
```

```{r}
kable(turnout_ethnicity_age_log_coefs(exit_poll_df, election_2016, census_data))
```

## Education

The coefficients of interest are:

```
logit(x) ~ (1 | education) + (1 | state:education) + 
           (non-education coefficients)
```

Similarly, education coefficients are determined thus using crude estimates for logistic regression coefficients. We estimate the number of voters with a level `ed` of education by taking the `options_perc - 2z*wilson_interval_width` proportion of the total votes cast (this is the assumption that the exit polls reflect the composition of voters). This lower bound of the Wilson interval is stored in the `lower_bound` which we use as the numerator of the logistic coefficient computation (the logarithm of the ratio of `ed` voters to `ed` non-voters).

### Overall Factor

We compute the `(1|education)` coefficient thus:

```{r}
turnout_education_log_coefs <- function(exit_polls, election_data, census_data, z = 2) {
  assert_that(all(is.state(exit_polls$state) || exit_polls$state == "nation"))
  assert_that(all(is.numeric(exit_polls$num_respondents)))
  assert_that(all(is.numeric(exit_polls$options_perc)))
  assert_that(all(is.numeric(census_data$education_high_school)))
  assert_that(all(is.numeric(census_data$education_some_college)))
  assert_that(all(is.numeric(census_data$education_college_grad)))
  assert_that(all(is.numeric(census_data$education_postgrad)))
  assert_that(all(is.state(census_data$state)))
  assert_that(all(has.electoral_delegates(election_data$state)))
  assert_that(all(is.numeric(election_data$totalvotes)))
  exit_polls %>%
    filter(questions=="Education", state=="nation") %>%
    group_by(options) %>%
    mutate(p = sum(0.01*options_perc*num_respondents)/sum(num_respondents),
           count = sum(num_respondents),
           width = wilson_width(p, count, z),
           total_votes = sum(election_data$totalvotes),
           lower_bound = (p - 2*z*width)*total_votes,
           high_school = sum(census_data$education_high_school),
           some_college = sum(census_data$education_some_college) + 0.5*sum(census_data$education_assoc_degree),
           college_grad = sum(census_data$education_college_grad) + 0.5*sum(census_data$education_assoc_degree),
           postgrad = sum(census_data$education_postgrad),
           proportion = (ifelse(options == "High school or less",
                                   min(0.99, lower_bound/high_school),
                                   ifelse(options == "Some college",
                                          min(0.99, lower_bound/some_college),
                                          ifelse(options == "College graduate",
                                                 min(0.99, lower_bound/college_grad),
                                                 ifelse(options == "Postgraduate",
                                                        min(0.99, lower_bound/postgrad),
                                                        NaN)))))) %>%
    transmute(beta_education = log(proportion/(1.0-proportion)) - beta_0,
              tau_education = 2*(4*width/(0.01*proportion))**-2) %>%
    unique
}
```

```{r}
kable(turnout_education_log_coefs(exit_poll_df, filter(election_2016, party=="democrat"), census_data))
```

The difficulty is there are more people in the exit polls claiming to be educated than the census data corroborates. Although postgrads are within the margin of error (roughly 21.9m people claim to be postgrads in the exit poll, whereas the census claims there should be 20.1m postgrads), the number of college graduates is off by 5m. I suspect people may consider receiving an associate's degree as being a "college graduate", which complicates the analysis. For simplicity, I suppose 50% of associate degrees call themselves college graduates, the other 50% say they have "some college" level of education.

### Interacting with State

We compute the `(1|education:state)` coefficient thus:

```{r}
turnout_education_state_log_coefs <- function(exit_polls, election_data, census_data, z = 2) {
  assert_that(all(is.state(exit_polls$state) || exit_polls$state == "nation"))
  assert_that(all(is.numeric(exit_polls$num_respondents)))
  assert_that(all(is.numeric(exit_polls$options_perc)))
  assert_that(all(is.numeric(census_data$education_high_school)))
  assert_that(all(is.numeric(census_data$education_some_college)))
  assert_that(all(is.numeric(census_data$education_college_grad)))
  assert_that(all(is.numeric(census_data$education_postgrad)))
  assert_that(all(is.state(census_data$state)))
  assert_that(all(has.electoral_delegates(election_data$state)))
  assert_that(all(is.numeric(election_data$totalvotes)))
  education_effects <- turnout_education_log_coefs(exit_polls, election_data, census_data, z)
  state_effects <- turnout_state_log_coefs(election_data, census_data)
  exit_polls %>%
    filter(questions=="Education", state!="nation") %>%
    mutate(options = factor(options)) %>%
    group_by(state) %>% # this group_by is responsible for the ":state" factor in `(1|education:state)`
    mutate(width = wilson_width(0.01*options_perc, num_respondents, z),
           center = wilson_center(0.01*options_perc, num_respondents, z),
           total_votes = sum(election_data[which(election_data$state %in% state & !is.na(election_data$totalvotes)),]$totalvotes),
           estimate_voters_by_education = (center + 2*z*width)*total_votes,
           high_school = sum(census_data[which(census_data$state %in% state),]$education_high_school),
           some_college = sum(census_data[which(census_data$state %in% state),]$education_some_college) + 0.5*sum(census_data[which(census_data$state %in% state),]$education_assoc_degree),
           college_grad = sum(census_data[which(census_data$state %in% state),]$education_college_grad) + 0.5*sum(census_data[which(census_data$state %in% state),]$education_assoc_degree),
           postgrad = sum(census_data[which(census_data$state %in% state),]$education_postgrad)
    ) %>%
    mutate(proportion = (ifelse(options == "High school or less",
                                min(0.99, estimate_voters_by_education/high_school),
                                ifelse(options == "Some college",
                                       min(0.99, estimate_voters_by_education/some_college),
                                       ifelse(options == "College graduate",
                                              min(0.99, estimate_voters_by_education/college_grad),
                                              ifelse(options == "Postgraduate",
                                                     min(0.99, estimate_voters_by_education/postgrad),
                                                     NaN)))))) %>%
    transmute(beta_education = log(proportion/(1.0-proportion)) - education_effects[which(education_effects$options==options),]$beta_education - state_effects[which(state_effects$state==state),]$beta_state - 3*beta_0,
              tau_education = 2*(4*width/(0.01*proportion))**-2,
              options) %>%
    arrange(state,options)
}
```

```{r}
kable(turnout_education_state_log_coefs(exit_poll_df, filter(election_2016, party=="democrat"), census_data))
```

Of course, we need to subtract out the `(1|education)` and `(1|state)` coefficients to avoid double counting effects. This ensures we capture the _state specific_ quirks due to education level.