---
title: "Kalman Filtering the Polls"
author: "Alex Nelson"
date: "7/5/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(lubridate)
library(truncnorm)
library(MASS)
library(kableExtra)
library(urbnmapr)
# library(ggthemes)

knitr::opts_chunk$set(echo = TRUE)
source("../R/states.R")
source("../R/presidential_elections.R")
```

# Introduction

Despite the name, a "Kalman filter" doesn't _filter_. It's more of a "guess-and-check"-er.

```{r}
polls_2016 <- read_csv("../data/polling/all_polls_2016.csv",
                       col_types = cols(
                           .default = col_character(),
  X1 = col_double(),
  start.date = col_date(format = ""),
  end.date = col_date(format = ""),
  entry.date.time..et. = col_datetime(format = ""),
  number.of.observations = col_double(),
  trump = col_double(),
  clinton = col_double(),
  other = col_double(),
  undecided = col_double(),
  question.iteration = col_double(),
  johnson = col_double(),
  mcmullin = col_double()
                       ))
```

## Simple filtering

So, the control vector would be the `entry.date.time..et.` date, because it's an external impulse.

```{r}
mi_2016 <- polls_2016 %>%
  filter(state == "MI") %>%
  arrange(entry.date.time..et.)

election_date_2016 <- ymd("2016-11-08")
```

We transform this into approximately normal variables

```{r}
mi_clinton <- mi_2016 %>%
  filter(partisan != "Pollster") %>%
  mutate(clinton_mu = (clinton/100),
         clinton_sd = qnorm(0.999)*sqrt(((clinton/100)*(1 - clinton/100) + (1 - (clinton+trump)/100)**2)/number.of.observations),
         t = 1+as.integer(end.date - min(mi_2016$end.date)))
```

```{r}
get_for_state <- function(state_str) {
  polls_2016 %>%
  filter(state == state_str) %>%
  arrange(entry.date.time..et.) %>%
  # filter(partisan != "Pollster") %>%
  mutate(clinton_mu = (clinton/100),
         sd = qnorm(0.999)*sqrt(((clinton/100)*(1 - clinton/100) + (1 - (clinton+trump)/100)**2)/number.of.observations),
         clinton_sd = sd,
         t = 1+as.integer(end.date - min(mi_2016$end.date)),
         precision = 1/clinton_sd**2)
}
```

```{r}
pool_polls <- function(df) {
  df %>%
    group_by(t) %>%
    summarize(clinton_pooled = weighted.mean(clinton/100, precision),
              trump_pooled = weighted.mean(trump/100, precision),
              trump = 100*trump_pooled,
              clinton = 100*clinton_pooled,
              sd = sqrt(1/sum(precision)),
              number.of.observations = sum(number.of.observations),
              end.date = first(end.date),
              number_of_polls = n()) %>%
    arrange(t)
}
```

This is a dummy Kalman filter running on a one-dimensional signal.

```{r}
kalman_filter <- function(df, mu, sd, election_dt = election_date_2016, h=1, f=1) {
  q = mean(df$sd)
  end_T <- as.integer(election_dt - min(df$end.date))
  x <- c(rnorm(1, 0.5, mean(df$sd)))
  p <- c(q)
  residual <- c()
  z <- rep(NA, end_T)
  r <- rep(NA, end_T)
  for (i in 1:nrow(df)) {
    z[df$t[i]] <- mu[i]
    r[df$t[i]] <- sd[i]
  }
  for (i in 1:end_T) {
    if (is.na(z[i])) {
      # random walk
      x <- c(x, rtruncnorm(1, a=0, b=1, mean=f*last(x), sd=q))
    } else {
      # predict
      x_pred <- f*last(x);
      p_pred <- f*last(p)*f + q;
      # correct
      y <- z[i] - h*x_pred;
      s <- h*p_pred*h + mean(r[1:i], na.rm=T);
      k <- p_pred*h/s;
      x <- c(x, x_pred + k*y)
      p <- c(p, (1 - k*h)*p_pred)
      residual <- c(residual, z[i] - h*last(x))
    }
  }
  list(x=x,
       p=p,
       residuals=residual)
}
```

```{r}
estimate_win <- function(df, n=100) {
  mean(replicate(last(kalman_filter(df, df$trump_pooled, df$sd)$x), n=n)  < replicate(last(kalman_filter(df, df$clinton_pooled, df$sd)$x), n=n))
}
```

# Refined Model

This time, lets use a multinomial distribution to describe a poll, then use the (multivariate) normal approximation which will be our `z[k]`. This will give us a normal variable `x[k][DEM]` for the estimated support for the Democratic candidate, and another `x[k][REP]` for the Republican candidate. Third party supporters become noise (more or less)...well, due to the quirkiness of the normal approximation to the multinomial, it's singular in nature (meaning: we can write `third_party = 1 - dem - rep` for the proportions), so we have an extra variable (`third_party`) that's not needed. (Just like real life.)


```{r}
multinomial_covariance_mat <- function(prob_vec, size) {
  mat <- matrix(data = -prob_vec %*% t(prob_vec),
                nrow=length(prob_vec))*size;
  diag(mat) <- prob_vec*(1 - prob_vec)*size;
  mat
}

norm_approx <- function(poll_df, rep, dem) {
  democrat <- enquo(dem)
  republican <- enquo(rep)
  poll_df %>%
    filter(!is.na(number.of.observations)) %>%
    transmute(p_dem = !!democrat/100,
              p_rep = !!republican/100,
              p_undecided = ifelse(is.na(undecided), 0.5*(1 - !!democrat/100 - !!republican/100), undecided/100),
              p_third = ifelse(is.na(undecided), 0.5*(1 - !!democrat/100 - !!republican/100), 1 - !!democrat/100 - !!republican/100 - undecided/100),
              precision = (qnorm(0.975)*0.5/sqrt(number.of.observations))**-2,
              start_date = start.date,
              end_date = end.date,
              published = as.Date(entry.date.time..et.),
              sample_size = number.of.observations,
              state = state,
              population = population
              ) %>%
    group_by(state,end_date) %>%
    summarize(p_dem = max(0, weighted.mean(p_dem, precision)),
              p_rep = max(0, weighted.mean(p_rep, precision)),
              p_undecided = max(0, weighted.mean(p_undecided, precision)),
              p_third = max(0, weighted.mean(p_third, precision)),
              precision = sum(precision),
              size = sum(sample_size),
              mu_dem = size*p_dem,
              mu_rep = size*p_rep,
              mu_undecided = size*p_undecided,
              mu_third = size*p_third,
              .groups = "drop")
}
```

Now, the `x_hat[k, k-1]` will need to be re-scaled to match the polling results, which is what happens with the `H[k]` matrix.

```{r}
make_h <- function(new_size, existing_size) {
  diag(new_size/existing_size, ncol=2, nrow=2)
}
```


```{r}
full_kalman <- function(df, election_date) {
  end_T <- as.integer(election_date - min(df$end_date))
  xs <- list()
  xs[[1]] <- (matrix(data=c(1600,1400,300,300), ncol=1))
  covariance_matrices <- list()
  covariance_matrices[[1]] <- 10000*matrix(data=c(50,-1/10,-1,-1,
                                            -1/10,50,-1,-1,
                                            -1,-1,5,1,
                                            -1,-1,1,5), nrow=4)
  residuals <- list()
  p <- list()
  
  
  for (i in 1:nrow(df)) {
    # estimate
    x_est <- last(xs)
    
    Q_mat <- diag((qnorm(0.975)**2)*0.25*df$size[i], 4);
    if (i > 1) {
      Q_mat <- Q_mat + multinomial_covariance_mat(last(p), size=sum(last(xs)))
    }
    P_est <- last(covariance_matrices) + Q_mat
    
    # predict
    z <- matrix(data=c(df$mu_dem[i], df$mu_rep[i], df$mu_third[i], df$mu_undecided[i]), ncol=1)
    
    h <- diag(1, nrow=4)
    if (i>1) {
      h <- diag(df$size[i]/sum(x_est), 4)
    }
    
    
    y <- z - (h %*% x_est)
    
    R_mat <- multinomial_covariance_mat(c(df$p_dem[i],
                                          df$p_rep[i],
                                          df$p_third[i],
                                          df$p_undecided[i]),
                                        size  = df$size[i]) +
      diag((qnorm(0.975)**2)*0.25*df$size[i], 4);
    R_mat[4,c(1,2,3)] <- 0; R_mat[c(1,2,3),4] <- 0;
    s <- (h %*% P_est %*% t(h)) + R_mat
    kalman_gain <- P_est %*% t(h) %*% solve(s)
    
    x_hat <- matrix(data=(x_est + (kalman_gain %*% y)), ncol=1)
    xs[[length(xs)+1]] <- x_hat
    p[[length(p) + 1]] <- x_hat/sum(x_hat)
    
    P_mat <- (diag(1, 4) - (kalman_gain %*% h)) %*% P_est %*% t((diag(1, 4) - (kalman_gain %*% h)))
    P_mat <- P_mat + (kalman_gain %*% R_mat %*% t(kalman_gain))
    covariance_matrices[[length(covariance_matrices)+1]] <- P_mat
    
    residuals[[length(residuals)+1]] <- (z - (h %*% x_hat))
  }
  
  list(x = xs,
       p_mat = covariance_matrices,
       prob = p,
       residuals = residuals)
}
```

We then can transform the result into a probability of winning. We need to estimate for a given state how its undecideds and third party supporters vote on election day. For now, using the 2016 outcome naively seems fine (wrong, but fine):



```{r}
state_results <- load_obj(state_path)
results_2016 <- filter(state_results, year==2016) %>%
  group_by(candidate,state) %>%
  summarize(candidatevotes = sum(candidatevotes),
            totalvotes = max(totalvotes),
            percent = candidatevotes/totalvotes) %>%
  filter(candidate %in% c("Trump, Donald J.", "Clinton, Hillary")) %>%
  dplyr::select(-candidatevotes,-totalvotes) %>%
  spread(candidate, percent) %>%
  rename(republican = `Trump, Donald J.`,
         democrat = `Clinton, Hillary`) %>%
  mutate(third_party = 1 - republican - democrat)

est_c_for_state <- function(state_abbrev) {
  estimates <- c(last(filter_state(state_abbrev, polls_2016, election_date_2016, dem=clinton)$prob))
  names(estimates) <- c('democrat', 'republican', 'third_party', 'undecided')
  state_2016 <- filter(results_2016, state == expand_abbrev[[state_abbrev]]) %>%
    dplyr::select(-state) %>%
    mutate(undecided=0)
  delta <- (state_2016 - estimates)/(estimates[['third_party']] + estimates[['undecided']] - state_2016$third_party)
  c(1,
    -1,
    (delta[['democrat']]-delta[['republican']])*abs(delta[['third_party']]),
    (delta[['democrat']]-delta[['republican']])*abs(delta[['undecided']]))
}
```
We then take our results, transform the multivariate normal distribution into a univariate normal distribution using the estimates we just considered:

```{r}
prob_dem_win <- function(results, state_abbrev=NA) {
  mu <- last(results$x)
  sigma <- last(results$p_mat)
  
  if (is.na(state_abbrev)) {
    c <- matrix(data = c(1,-1,-1/3,-1/3), ncol=1)
    # c <- matrix(data = c(1,-1,0.345*(-0.1790679),-0.1790679), ncol=1)
  } else {
    c <- est_c_for_state(state_abbrev)
  }
  
  s2 <- t(c) %*% sigma %*% c
  pnorm(0.5, t(c) %*% mu, sd=sqrt(abs(s2)), lower.tail=F)
}
```

```{r}
mean_sd <- function(results) {
  s <- 0
  for(x in results$x) {
    s <- s + x
  }
  mu <- s/length(results$x)
  s2 <- 0
  for(x in results$x) {
    s2 <- s2 + (x - mu)**2
  }
  list(var = s2/length(results$x),
       mean = mu)
}
```

Iterate through the states with polls available "this far out" from election day back in 2016, and save the probability the Democrat would win for each state.

```{r}
run_2016 <- function(end_dt = today() - years(4)) {
  results <- list();
  for (state in unique(polls_2016$state)) {
    if (state != "--") {
      polls <- norm_approx(get_for_state(state) %>% 
                              filter(end.date <= end_dt),
                            dem=clinton, rep=trump);
      if (nrow(polls) > 1) {
        results[[state]] <- prob_dem_win(full_kalman(polls, election_date_2016))
      }
    }
  }
  results;
}
```

Now we save the electoral delegates to the most likely winner for each state.

```{r}
expected_ev <- function(forecast) {
  acc <- list(dem = 0, rep = 0);
  for (state in names(forecast)) {
    if (is.na(forecast[[state]])) {
      
    } else {
      party <- ifelse(forecast[[state]] < 0.5, "rep", "dem")
      acc[[party]] <- acc[[party]] + electoral_delegates.abbrev[[state]];
    }
  }
  acc;
}
```

# Projecting for 2020

Now, we can move forward and start projecting for 2020. First a helper function to download the latest polls.

```{r}
polls_2020_url <- "https://docs.google.com/spreadsheets/d/e/2PACX-1vQ56fySJKLL18Lipu1_i3ID9JE06voJEz2EXm6JW4Vh11zmndyTwejMavuNntzIWLY0RyhA1UsVEen0/pub?gid=0&single=true&output=csv"


download_latest_2020_polls <- function() {
  if(file.exists("./data/polling/all_polls_2020.csv")) {
    file.rename("./data/polling/all_polls_2020.csv",
                paste0("./data/polling/all_polls_2020.",
                       as.numeric(now()),
                       ".csv"))
  }
  download.file(polls_2020_url, 
              "./data/polling/all_polls_2020.csv", method = "auto")
  # if the file has data (i.e., we succeeded in downloading it)
  # then rename it to "all_polls_2020.csv"
}
```

```{r}
polls_2020 <- read_csv("../data/polling/all_polls_2020.csv",
                       col_types = cols(
                           .default = col_character(),
  start.date = col_date(format = "%m/%d/%Y"),
  end.date = col_date(format = "%m/%d/%Y"),
  entry.date.time..et. = col_datetime(format = "%m/%d/%Y %H:%M:%S"),
  number.of.observations = col_double(),
  trump = col_double(),
  biden = col_double(),
  other = col_double(),
  undecided = col_double()
                       ))
```

Lets examine the frequencies

```{r}
kable(polls_2020 %>% group_by(state) %>% summarize(count = n()) %>% arrange(-count))
```


A helper function to automatically run the Kalman filter on a state:

```{r}
filter_state <- function(state_name, polls, election_date, dem=biden, rep=trump) {
  democrat <- enquo(dem)
  republican <- enquo(rep)
  full_kalman(norm_approx(polls %>% filter(state == state_name),dem=(!!democrat), rep=(!!republican)),
              election_date)
}
```


We can now automate running the Kalman filter for the states:


```{r}
election_date_2020 <- ymd("2020-11-03")

run_2020 <- function() {
  results <- list();
  for (state_name in unique(polls_2020$state)) {
    if (state_name != "--") {
      polls <- norm_approx(polls_2020 %>% filter(state == state_name),dem=biden, rep=trump);
      if (nrow(polls) > 0) {
        results[[state_name]] <- prob_dem_win(full_kalman(polls, election_date_2020), state_name)
      }
    }
  }
  results;
}
```

## Words of Estimative Probability

The ratings may be estimated qualitatively as "Safe", "Likely", "Lean", "Toss up" (and sometimes "Tilt"). Based off of G. Elliott Morris' [article](http://centerforpolitics.org/crystalball/articles/two-ways-of-thinking-about-election-predictions-and-what-they-tell-us-about-2018/):

> Each of these categories also has a corresponding Democratic probability of victory for the seats placed within. In Lean Democratic seats, Democrats win the elections 78% of the time; Lean Republican: 18%; Likely D: 95%; Likely R: 3%; Safe D: 99%; Safe R 0%, and Toss-up districts: 59%. These values are plotted on the right of the preceding figure, with the size of each point showing the number of seats earning that designation over the years.



```{r}
ratings <- list(
  lean_dem = 0.8,
  lean_rep = 0.2,
  likely_dem = 0.95,
  likely_rep = 0.05,
  safe_dem = 0.99,
  safe_rep = 0.01,
  toss_up = 0.5,
  # interpolated using geometric mean
  tilt_dem = 0.65,
  tilt_rep = 0.35
)

qualitative_label <- function(prob_dem_win) {
  # original ratings estimate
  # ratings <- list(lean_dem = 0.78,
  #                 lean_rep = 0.18,
  #                 likely_dem = 0.95,
  #                 likely_rep = 0.03,
  #                 safe_dem = 0.99,
  #                 safe_rep = 0,
  #                 toss_up = 0.59)

  delta <- 1
  label <- NA
  for (rating in names(ratings)) {
    tmp <- abs(prob_dem_win - ratings[[rating]])
    if (tmp < delta) {
      delta <- tmp
      label <- rating
    }
  }
  label  
}
```

We can then print out the qualitative estimates for an election as:

```{r}
dump_forecast <- function(prediction) {
  states <- c()
  labels <- c()
  for (state in sort(names(prediction))) {
    print(state)
    if (!is.na(prediction[[state]])) {
      states <- c(states, state)
      labels <- c(labels, qualitative_label(prediction[[state]]))
    }
  }
  data.frame(state_abbv = states,
             prediction = labels)
}
```

```{r}
plot_prediction <- function(df) {
  spatial_data <- left_join(get_urbn_map(map = "states", sf = TRUE),
                            df,
                            by = "state_abbv")
  spatial_data
    p <- ggplot() +
    geom_sf(
      sdf1,
      mapping = aes(fill = probability),
      color = "#ffffff",
      size = 0.25) +
    labs(fill = "Homeownership rate")
  return(spatial_data)
}

```


```{r}
f1 <- run_2020()
f0 <- dump_forecast(f1)
current_forecast <- rbind(f0,
             data.frame(state_abbv = names(electoral_delegates.abbrev),
                        prediction = rep("no_data", 51)) %>%
               filter(!(state_abbv %in% f0$state_abbv))) %>%
  mutate(prediction = factor(ifelse(is.na(prediction), "toss_up", prediction),
                             ordered=T,
                             levels=c("safe_rep",
                                      "likely_rep",
                                      "lean_rep",
                                      "tilt_rep",
                                      "toss_up",
                                      "tilt_dem",
                                      "lean_dem",
                                      "likely_dem",
                                      "safe_dem",
                                      "no_data"))) %>%
  arrange(state_abbv)

sdf1 <- inner_join(get_urbn_map(map = "states", sf=T),
                  current_forecast,
                  by="state_abbv")

color_code <- list("safe_rep" = "#800000",
                   "likely_rep" = "#ff0000",
                   "lean_rep" =  "#FF6347",
                   "tilt_rep" =  "#DB7093",
                   "toss_up" = "#daa520", # "#a9a9a9",
                   "tilt_dem" = "#ADD8E6",
                   "lean_dem" = "#00BFFF", 
                   "likely_dem" = "#1E90FF",
                   "safe_dem" =  "#00008B",
                   "no_data" = "#DCDCDC")

# ggthemes::colorblind_pal()(5)
plot <- ggplot() +
  geom_sf(sdf1, mapping = aes(fill = prediction),
          color = "#ffffff") +
  scale_fill_manual("Rating", values = unlist(color_code[sort(unique(current_forecast$prediction))])) +
  labs(fill = "Rating",
       title = "Kalman Filtered Poll Aggregator",
       subtitle = paste0("State-level Presidential polls applied to a Kalman Filter, ",
                         today()),
       caption = "https://political-arithmetic.blogspot.com") +
  coord_sf(datum = NA)

outfile <- sprintf("../img/2020-plots/kalman-filter-%s.png",today())
ggsave(outfile, plot=plot)
plot
```

```{r}
source("../R/presidential_elections.R")
source("../R/states.R")
state_results <- load_obj(state_path)
state_results_2016 <- state_results %>%
  filter(year==2016, candidate=="Clinton, Hillary") %>%
  group_by(state_po) %>%
  summarize(results = sum(candidatevotes)/max(totalvotes)) %>%
  rename(state = state_po)
econ_predict_2016 <- read_rds("../data/elections/presidential/pct_clinton_full.rds")
naive_econ_predict_2016 <- read_rds("../data/elections/presidential/pct_clinton_april_1.rds")

df <- left_join(econ_predict_2016 %>%
                   filter(state != "--"),
                 state_results_2016,
                 by = "state") %>%
  mutate(term = results*log2(results/mean) + (1 - results)*log2((1 - results)/(1 - mean))) 

naive_deltas <- left_join(naive_econ_predict_2016 %>%
                   filter(state != "--", t==max(t)),
                 state_results_2016,
                 by = "state") %>%
  group_by(state) %>%
  summarize(diff = (mean - results))
deltas <- left_join(econ_predict_2016 %>%
                   filter(state != "--", t==max(t)),
                 state_results_2016,
                 by = "state") %>%
  group_by(state) %>%
  summarize(diff = (mean - results))
```

```{r}
df0 <- data.frame(states = as.character(df$state),
                  term = df$term)
df0$ev <- unlist(electoral_delegates.abbrev[df0$states])
df0$loss <- df0$term * df0$ev
#  mutate(ev = electoral_delegates.abbrev[states])
```

```{r}
us_est <- c(last(filter_state("--", polls_2016, election_date_2016, dem=clinton)$prob))
names(us_est) <- c('democrat', 'republican', 'third_party', 'undecided')
us_2016 <- c(65853514, 62984828,7830934,0)/136669276
#tx_2016 <- filter(results_2016, state=="Texas") %>%
#  dplyr::select(-state) %>%
#  mutate(undecided=0)
```