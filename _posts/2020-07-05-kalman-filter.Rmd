---
title: "Kalman Filtering the Polls"
author: "Alex Nelson"
date: "7/5/2020"
output: 
  md_document:
    variant: gfm
    toc: true
    toc_depth: 4
---

```{r setup, include=FALSE}
library(matrixcalc)
library(tidyverse)
library(lubridate)
library(truncnorm)
library(MASS)
library(kableExtra)
library(urbnmapr)
library(ggfortify)
# library(ggthemes)

knitr::opts_chunk$set(echo = TRUE)
source("../R/states.R")
source("../R/presidential_elections.R")
```

# Introduction

Despite the name, a "Kalman filter" doesn't _filter_. It's more of a "guess-and-check"-er.

```{r}
polls_2016_raw <- read_csv("../data/polling/all_polls_2016.csv",
                       col_types = cols(
                           .default = col_character(),
  X1 = col_double(),
  start.date = col_date(format = ""),
  end.date = col_date(format = ""),
  entry.date.time..et. = col_datetime(format = ""),
  number.of.observations = col_double(),
  trump = col_double(),
  clinton = col_double(),
  other = col_double(),
  undecided = col_double(),
  question.iteration = col_double(),
  johnson = col_double(),
  mcmullin = col_double()
                       ))
```

Then we shadow an identifier which we'll use shortly but define later on:

```{r}
polls_2016 <- polls_2016_raw # XXX fix this, declaration ahead of time
```

# Multinomial Asymptotically Normal Approximation

This time, lets use a multinomial distribution to describe a poll, then use the (multivariate) normal approximation which will be our `z[k]`. This will give us a normal variable `x[k][DEM]` for the estimated support for the Democratic candidate, and another `x[k][REP]` for the Republican candidate. Third party supporters become noise (more or less)...well, due to the quirkiness of the normal approximation to the multinomial, it's singular in nature (meaning: we can write `third_party = 1 - dem - rep` for the proportions), so we have an extra variable (`third_party`) that's not needed. (Just like real life.)

We have the multinomial covariance matrix used for, well, lots of things. It's asymptotically the multivariate normal distribution's covariance matrix, so we need to compute it frequently.

```{r}
multinomial_covariance_mat <- function(prob_vec, size) {
  mat <- matrix(data = -prob_vec %*% t(prob_vec),
                nrow=length(prob_vec))*size;
  diag(mat) <- prob_vec*(1 - prob_vec)*size;
  mat
}
```

## Combining Data

Following Jackman's "Pooling the Polls", we take a precision-weighted average of polls concluding on the same day. This amounts to, for a single poll released on a given end date, renaming the variables and computing the covariance and precision matrices. For multiple polls released on a given day, taking `matrix.inverse(matrix.sum(precision_matrix)) %*% (precision_matrix[[1]] %*% (poll_size[[1]]*poll_proportions[1]) + ... + precision_matrix[[N]] %*% (poll_size[[N]] * poll_proportions[N]))`. This is because, unlike Jackman, we are treating polling data as a 4-vector (as opposed to a scalar tracking support for one particular party or candidate).

We have slight nuances, like factoring in House Effects for pollsters (as impacting the covariance matrix), but we're fairly faithful to Jackman's original intentions.

```{r}
# forward declaration...
parse_house_effect <- function(house_effect) {
  if (is.na(house_effect)) { return(matrix(c(0,0,0,0), ncol=1)); }
  
  if ("D" == substring(house_effect, 1, 1)) {
    delta <- 0.5*(parse_number(house_effect))
    return(matrix(c(-delta, delta, 0, 0)/100, ncol=1));
  } else {
    delta <- 0.5*(parse_number(house_effect))
    return(matrix(c(delta, -delta, 0, 0)/100, ncol=1));
  }
}

use_house_effect <- F

# helper function used only once
make_symmetric_matrix <- function(mat) {
  assertthat::assert_that(is.matrix(mat))
  assertthat::assert_that(is.square.matrix(mat))
  result <- mat;
  if (!is.symmetric.matrix(result)) {
    tmp <- 0.5*mat
    result <- as.matrix(tmp + t(tmp))
  }
  assertthat::assert_that(is.symmetric.matrix(result))
  return(result)
}

make_positive_definite <- function(mat) {
  assertthat::assert_that(is.matrix(mat))
  assertthat::assert_that(is.square.matrix(mat))
  if (is.symmetric.matrix(mat) && is.positive.definite(mat)) {
    return(mat)
  }
  
  result <- make_symmetric_matrix(mat)
  if (is.positive.definite(result)) {
    return(result);
  }

  eigen_vs <- eigen(result)
  result <- make_symmetric_matrix(eigen_vs$vectors %*% diag(ifelse(eigen_vs$values < 1e-5, 1, eigen_vs$values), nrow=nrow(mat),ncol=ncol(mat)) %*% solve(eigen_vs$vectors))
  
  assertthat::assert_that(is.matrix(result),
                          msg = sprintf("Somehow, the result is not a matrix? It's a %s", class(result)))
  assertthat::assert_that(is.square.matrix(result))
  assertthat::assert_that(is.positive.definite(result))
  return(result)
}
```

We also need to handle the cases where the poll doesn't ask about third party voters.

```{r}
make_nonsingular <- function(cov_mat, size) {
  if(!is.singular.matrix(cov_mat)) {
    return(cov_mat)
  }
  eg <- eigen(cov_mat)
  min_val <- rep(qnorm(0.975)*sqrt(size), length(eg$values))
  
  eg$vectors %*% diag(ifelse(eg$values<1e-7, min_val, eg$values), length(eg$values)) %*% solve(eg$vectors)
}
```

## Determining Dates for Polls

We need to assign a date for polls. Sadly polling requires multiple consecutive days, so which date to pick? We use the mid-date.

```{r}
poll_date <- function(start_date, end_date) {
  as.Date(start_date + 0.5*(end_date - start_date))
}

mid_date <- function(intvl) {
  poll_date(int_start(intvl), int_end(intvl))
}
```

We also need to weigh the covariance matrix by the duration of the polling process. Computing the length of the polling process requires adding the end-date to its duration (since there are polls which are 1-day long, whose start date _is_ its end date).

```{r}
poll_length_in_days <- function(start_date, end_date) {
  assertthat::assert_that(is.Date(start_date))
  assertthat::assert_that(is.Date(end_date))
  assertthat::assert_that(start_date <= end_date)
  result <- 1 + as.numeric(end_date - start_date)
  assertthat::assert_that(result > 0)
  return(result)
}
```

We then have a helper function to conjoin the date and duration of the poll.

```{r}
conjoin_date_info <- function(polls) {
  polls %>% mutate(
    date = poll_date(start.date, end.date),
    length = poll_length_in_days(start.date, end.date)
  )
}
```

## Pooling the Polls

```{r}
pool_single_poll <- function(data, dt) {
  min_probability <- 0.5*qnorm(0.975)/sqrt(data$sample_size)
  prob_data <- rescale_probabilities(
    matrix(c(max(data[[1,'p_dem']], 0),
             max(data[[1,'p_rep']], 0),
             max(data[[1,'p_third']], 0),
             max(data[[1,'p_undecided']], 0)),
           ncol=1) +
      use_house_effect*parse_house_effect(data$house_effect),
    min_probability)#/sqrt(data$sample_size)

  cov_mat <- make_positive_definite((make_nonsingular(multinomial_covariance_mat(matrix(data = prob_data, ncol=1), data$sample_size), data$sample_size) +
      diag(qnorm(0.975)*sqrt(data$sample_size), 4)) +
      diag(sqrt(data$sample_size)*0*ifelse(is.na(data[[1, "predictive"]]),
                                         1,
                                         data[[1, "predictive"]])/100,4))*(data[[1, "poll_length"]])
  
  # prob_data <- c(data[[1,'p_dem']],data[[1,'p_rep']],data[[1,'p_third']],data[[1,'p_undecided']]) # + use_house_effect*c(parse_house_effect(data$house_effect))#/sqrt(data$sample_size)
  # prob_data <- c(data[[1,'p_dem']],data[[1,'p_rep']],data[[1,'p_third']],data[[1,'p_undecided']]) + use_house_effect*c(parse_house_effect(data$house_effect))/sqrt(data$sample_size)
  
  assertthat::assert_that(is.positive.definite(cov_mat),
                          msg="Pooling a single poll produces a covariance matrix that is not positive definite...strange...")
  
  return(list(p_dem = prob_data[[1]], 
              p_rep = prob_data[[2]],
              p_third = prob_data[[3]],
              p_undecided = prob_data[[4]],
              size = data$sample_size,
              mu_dem = prob_data[[1]]*data$sample_size,
              mu_rep = prob_data[[2]]*data$sample_size,
              mu_third = prob_data[[3]]*data$sample_size,
              mu_undecided = prob_data[[4]]*data$sample_size,
              precision = MASS::ginv(cov_mat),
              covariance = cov_mat,
              date = dt))
}
```

```{r}
rescale_probabilities <- function(probabilities, min_probability) {
 # return(probabilities)
  if (all(probabilities >= min_probability)) {
    return(probabilities);
  }
  num_zeroes <- sum(probabilities < min_probability)
  # print(num_zeroes)
  recaled <- (1 - num_zeroes*min_probability)/sum(ifelse(probabilities < min_probability, 0, probabilities))
  
  
  matrix(ifelse(probabilities < min_probability, min_probability, probabilities*recaled),
         ncol=1)
}
```

The real complexity lies in pooling multiple polls. We compute the covariance matrix for each poll, invert it to find the precision matrix, and keep a running sum of precision matrices.

Note, we scale the covariance by the poll length. We expect `n/poll_length` people sampled each day, for a poll of sample size `n`. This translates to a covariance matrix scaled by `poll_length`.

```{r}
pool_multiple_polls <- function(data, dt) {
  #print(dt)
  #print(data)
  prob_numerator <- matrix(c(0,0,0,0), ncol=1)
  mu_numerator <- matrix(c(0,0,0,0), ncol=1)
  precision <- list()
  denominator <- diag(0, 4)
  probabilities <- list()
  
  for (i in 1:nrow(data)) {
    size <- data[[i, 'sample_size']]
    min_prob <- 0*qnorm(0.975)/sqrt(size)
    prob_data <- # rescale_probabilities(
      matrix(c(max(data[[i,'p_dem']], 0),
               max(data[[i,'p_rep']], 0),
               max(data[[i,'p_third']], 0),
               max(data[[i,'p_undecided']], 0)),
             ncol=1) + use_house_effect*c(parse_house_effect(data[[i,'house_effect']]))
      #min_prob)# /sqrt(size)
    probabilities[[length(probabilities)+1]] <- prob_data
    
    p_raw <- c(max(data[[i,'p_dem']], 0),
               max(data[[i,'p_rep']], 0),
               max(data[[i,'p_third']], 0),
               max(data[[i,'p_undecided']], 0))
    #print("P_RAW = ")
    #print(p_raw)
    tmp <- rescale_probabilities(
      matrix(ifelse(p_raw<1e-7, min_prob, p_raw),
             ncol=1) +
        use_house_effect*c(parse_house_effect(data[[i,'house_effect']])),
      min_prob)
    tmp <- p_raw
    
    covariance_mat <- make_positive_definite(
      make_nonsingular(multinomial_covariance_mat(matrix(data = tmp, ncol=1),
                                                  ifelse(is.na(data[[i, "predictive"]]), size, size)) +
        diag(sqrt(size)*ifelse(is.na(data[[i, "predictive"]]),
                               qnorm(0.975)/100,
                               data[[i, "predictive"]]/100),
             4), size)  # +
        # diag(qnorm(0.975)*sqrt(size), 4)
      )*(data[[i, "poll_length"]]^2)
    
    
    p_mat <- tryCatch(solve(covariance_mat),
                      error=function(e) {
                        MASS::ginv(covariance_mat)
                      })
    
    denominator <- denominator + p_mat
    precision[[length(precision)+1]] <- p_mat
    prob_numerator <- prob_numerator + p_mat %*% prob_data
    mu_numerator <- (mu_numerator + (p_mat %*% (prob_data*size)))
    #print(covariance_mat)
    #print(p_mat)
    #print(p_mat %*% (prob_data*size))
  }
  den <- denominator

  cov_mat <- make_positive_definite(
    tryCatch(solve(den),
             error=function(e) {
               eigen_den <- eigen(den)
               vs <- eigen_den$values**-1
               vs[[4]] <- exp(mean(log(vs[1:3])))
               eigen_den$vectors %*% diag(vs,4) %*% solve(eigen_den$vectors)
             }))
  
  mu <- abs(cov_mat %*% mu_numerator)
  size <- sum(mu)
  assertthat::assert_that(is.positive.definite(cov_mat),
                          msg="Pooling multiple polls produces a covariance matrix that is not positive definite")
  assertthat::assert_that(size > 0)
  assertthat::assert_that(!any(mu < 0))
  return(list(p_dem = (mu[[1]]/size),
              p_rep = (mu[[2]]/size),
              p_third = (mu[[3]]/size),
              p_undecided = (mu[[4]]/size),
              mu_dem = (mu[[1]]),
              mu_rep = (mu[[2]]),
              mu_third = (mu[[3]]),
              mu_undecided = (mu[[4]]),
              size = (size),
              precision = denominator,
              covariance = cov_mat,
              date = dt
  ))
}

pool_polls <- function(data, dt) {
  if (nrow(data) == 1) {
    df0 <<- data;
    dt0 <<- dt;
    return(pool_single_poll(data, dt))
  } else {
    df1 <<- data;
    dt1 <<- dt;
    return(pool_multiple_polls(data, dt))
  }
}
```

```{r}
renormalize_probability <- function(probabilities, size) {
  min_probability <- qnorm(0.975)/sqrt(size)
  if (all(probabilities >= min_probability)) {
    return(probabilities);
  }
  num_zeroes <- sum(probabilities < min_probability)
  print(num_zeroes)
  recaled <- (1 - num_zeroes*min_probability)/sum(ifelse(probabilities < min_probability, 0, probabilities))
  
  ifelse(probabilities < min_probability, min_probability, probabilities*recaled)
}

test_probs <- function() {
  # renormalize_probability(c(0.47-0.021, 0.44+0.021, 0.045, 0.045), 3070)
  renormalize_probability(c(0.51-0.009, 0.45+0.009, 0.02, 0.02), 1234)
}
# norm_approx(filter(polls_2020, state=="MI"), trump, biden)
```

We now use the normal approximation to a poll treated as a realization of a multinomial distribution. We can also take advantage of pooling the polls, so any polls concluding on the same day can be combined together to decrease error.

A note about the parameters expected to the `norm_approx` function, it expects the column names for the Republican candidate (`rep`) and Democratic candidate (`dem`). We use it by simply calling `norm_approx(polls_2020, trump, biden)` for example, no need for quotes or anything of the sort.

```{r}
norm_approx <- function(poll_df, rep, dem) {
  poll_df_tmp <<- poll_df
  democrat <- enquo(dem)
  republican <- enquo(rep)
  
  df <- poll_df %>%
    filter(!is.na(number.of.observations),
           !is.na(end.date),
           !is.na(start.date))
  
  # dplyr mutate doesn't work with my functions :'(
  df$poll_len <- as.integer(df$end.date - df$start.date)+1
  df$dt <- round_date(df$start.date + 0.5*(df$end.date - df$start.date),
                      unit = "day")
  
  df2 <<- df %>%
    transmute(date = dt,
              poll_length = poll_len,
              p_dem = !!democrat/100,
              p_rep = !!republican/100,
              p_undecided = ifelse(is.na(undecided) | (undecided + !!republican + !!democrat + ifelse(is.na(other), 0, other) != 100),
                                   ifelse(is.na(other),
                                          0.95*(1 - !!democrat/100 - !!republican/100),
                                          (1 - !!democrat/100 - !!republican/100 - other/100)),
                                   undecided/100),
              p_third = ifelse(!is.na(other),
                               other/100,
                               ifelse(is.na(undecided),
                                      0.05*(1 - !!democrat/100 - !!republican/100),
                                      1 - !!democrat/100 - !!republican/100 - undecided/100)
                               ),
              sample_size = number.of.observations,
              house_effect = house_effect,
              predictive = predictive) %>%
    arrange(date)
  
  df2  %>%
    nest_by(date) %>%
    mutate(data = list(pool_polls(data, date)))
}
```

# Kalman Filter

The Kalman filter assumes we have latent variables `x[t]` (like the electorate's support for candidates) which are observed through sampling `y[t]` (e.g., with polls). The basic equations for this system is `x[t+1] = F[t]%*%x[t] + v[t]` where `v[t] ~ mvnorm(0, q_mat[t])`, and `y[t] = H[t] %*% x[t] + w[t]` for `w[t] ~ mvnorm(0, r_mat[t])`. The units for `x[t]` and `y[t]` are respondents to the poll who support the specified candidate. We estimate `x[t]` as `x_hat[t]`. 

Now, the `x_hat[k, k-1]` will need to be re-scaled to match the polling results, which is what happens with the `H[k]` matrix.

```{r}
make_h <- function(new_size, existing_size) {
  diag(new_size/existing_size, ncol=2, nrow=2)
}
```

The big moment: Kalman filter the polling data (as a multivariate normally distributed sample).

```{r}
full_kalman <- function(data, election_date) {
  end_T <- as.integer(election_date - min(data$date))
  xs <- list()
  xs[[1]] <- (matrix(data=c(1622,1497,315,166), ncol=1))
  covariance_matrices <- list()
  covariance_matrices[[1]] <- 12*multinomial_covariance_mat(xs[[1]]/sum(xs[[1]]), size=sum(xs[[1]]))
  diag(covariance_matrices[[1]]) <- 5*diag(covariance_matrices[[1]])
  residuals <- list()
  p <- list()
  
  dates <- list()
  for (i in 1:nrow(data)) {
    df <- data$data[[i]]
    dates[[1+length(dates)]] <- df$date
    
    # estimate
    x_est <- last(xs)
    
    Q_mat <- diag((qnorm(0.975)**2)*0.25*df$size, 4);
    if (i > 1) {
      # random walk suggests we multiply by the days between the end dates
      days_between_polls <- as.numeric(dates[[length(dates)]] - dates[[length(dates)-1]])
      Q_mat <- (Q_mat + multinomial_covariance_mat(last(p), size=sum(last(xs))))*ifelse(is.na(days_between_polls), 1, days_between_polls)
    }
    P_est <- last(covariance_matrices) + Q_mat
    
    # predict
    z <- matrix(data=c(df$mu_dem, df$mu_rep, df$mu_third, df$mu_undecided), ncol=1)
    
    h <- diag(1, nrow=4)
    if (i>1) {
      h <- diag(df$size/sum(x_est), 4)
    }
    
    y <- z - (h %*% x_est)
    
    R_mat <- multinomial_covariance_mat(matrix(data=c(df$p_dem, df$p_rep, df$p_third, df$p_undecided), ncol=1), size=df$size) + df$covariance;
    # R_mat[4,c(1,2,3)] <- 0; R_mat[c(1,2,3),4] <- 0;
    s <- (h %*% P_est %*% t(h)) + R_mat
    kalman_gain <- P_est %*% t(h) %*% solve(s)
    
    x_hat <- matrix(data=(x_est + (kalman_gain %*% y)), ncol=1)
    xs[[length(xs)+1]] <- x_hat
    p[[length(p) + 1]] <- x_hat/sum(x_hat)
    
    P_mat <- (diag(1, 4) - (kalman_gain %*% h)) %*% P_est %*% t((diag(1, 4) - (kalman_gain %*% h)))
    P_mat <- P_mat + (kalman_gain %*% R_mat %*% t(kalman_gain))
    covariance_matrices[[length(covariance_matrices)+1]] <- P_mat
    
    residuals[[length(residuals)+1]] <- (z - (h %*% x_hat))
  }
  
  list(x = xs,
       p_mat = covariance_matrices,
       prob = p,
       residuals = residuals,
       date = dates)
}
```

We then can transform the result into a probability of winning. We need to estimate for a given state how its undecideds and third party supporters vote on election day. For now, using the 2016 outcome naively seems fine (wrong, but fine):



```{r}
election_date_2016 <- ymd('2016-11-08')
state_results <- load_obj(state_path)
results_2016 <- filter(state_results, year==2016) %>%
  group_by(candidate,state) %>%
  summarize(candidatevotes = sum(candidatevotes),
            totalvotes = max(totalvotes),
            percent = candidatevotes/totalvotes) %>%
  filter(candidate %in% c("Trump, Donald J.", "Clinton, Hillary")) %>%
  dplyr::select(-candidatevotes,-totalvotes) %>%
  spread(candidate, percent) %>%
  rename(republican = `Trump, Donald J.`,
         democrat = `Clinton, Hillary`) %>%
  mutate(third_party = 1 - republican - democrat)
```

```{r}
estimate_c <- function(results, predictions) {
  assertthat::assert_that((0.95 <= sum(results)),
                          msg = "sum(results) < 0.95")
  assertthat::assert_that((sum(results) <= 1 + .Machine$double.eps),
                          msg = "sum(results) > 1")
  assertthat::assert_that((0.95 <= sum(predictions)),
                          msg = "sum(predictions) < 0.95")
  assertthat::assert_that((sum(predictions) <= 1 + .Machine$double.eps),
                          msg = sprintf("(sum(predictions) = %f) > 1", sum(predictions)))
  
  
  delta_dem <- min(results$democrat/predictions$democrat, 1)
  delta_rep <- -min(results$republican/predictions$republican, 1)
  delta_undecided <- 0
  delta_third <- 0
  
  if (results$democrat > predictions$democrat) {
    bolted_republicans <- max(0, predictions$republican - results$republican)
    unexplained_dem <- results$democrat - predictions$democrat - bolted_republicans
    
    if (unexplained_dem > predictions$undecided) {
      delta_third <- (unexplained_dem - predictions$undecided)/(predictions$third_party)
      
      delta_undecided <- 1
    } else {
      delta_third <- (unexplained_dem/(predictions$undecided + abs(predictions$third_party - results$third_party)))
      # delta_undecided <- unexplained_dem/predictions$undecided
      delta_undecided <- delta_third
    }
  } else if (results$republican > predictions$republican) {
    bolted_dems <- max(0, predictions$democrat - results$democrat)
    unexplained_rep <- results$republican - predictions$republican - bolted_dems
    
    if (unexplained_rep > predictions$undecided) {
      delta_third <- -((unexplained_rep - predictions$undecided)/(abs(predictions$third_party)))
      
      delta_undecided <- -1
    } else {
      delta_third <- -(unexplained_rep/(predictions$undecided + abs(predictions$third_party - results$third_party)))
      # delta_undecided <- unexplained_dem/predictions$undecided
      delta_undecided <- delta_third
    }
  }
  
  c(delta_dem, delta_rep, delta_third, delta_undecided)
} 
```

Now, I'm paranoid whether we programmed this correctly or not. So let's consider some unit tests. What do we expect when the polls badly over-estimate the Democratic candidate and the Republican candidate had silent supporters?

```{r}
over_estimate_dem_test <- function() {
  estimates <- tibble(democrat = 0.65,
                      republican = 0.25,
                      third_party = 0.08,
                      undecided = 0.02)
  results <- tibble(democrat = 0.45,
                      republican = 0.5,
                      third_party = 0.05,
                      undecided = 0)
  
  c <- c(0.45/0.65, -1, (0.08-0.05)*-1/0.08, -1)
  guess <- estimate_c(results, estimates)
  assertthat::assert_that(all.equal(c, guess))
  T
}
```

Similarly, if the Republican party is over-estimated.

```{r}
over_estimate_rep_test <- function() {
  estimates <- tibble(democrat = 0.25,
                      republican = 0.65,
                      third_party = 0.08,
                      undecided = 0.02)
  results <- tibble(democrat = 0.5,
                      republican = 0.45,
                      third_party = 0.05,
                      undecided = 0)
  
  c <- c(1, -0.45/0.65, 3/8, 1)
  guess <- estimate_c(results, estimates)
  assertthat::assert_that(all.equal(c, guess))
  T
}
```

Now, if the third party is over-estimated, we should: sum the undecideds and the error in third-party forming the "ersatz undecideds", then form the ratio of the unexplained Democratic vote to this "ersatz undecideds":

```{r}
over_estimate_third_party_test <- function() {
  estimates <- tibble(democrat = 0.4,
                      republican = 0.4,
                      third_party = 0.1,
                      undecided = 0.1)
  results <- tibble(democrat = 0.5,
                      republican = 0.45,
                      third_party = 0.05,
                      undecided = 0)
  ersatz_undecided <- estimates$undecided + abs(results$third_party - estimates$third_party)
  unexplained_dem <- results$democrat - estimates$democrat
  c <- c(1, -1, unexplained_dem/ersatz_undecided, unexplained_dem/ersatz_undecided)
  
  guess <- estimate_c(results, estimates)
  assertthat::assert_that(all.equal(c, guess))
  T
}
```

We wrap all these unit tests into a single function:

```{r}
estimate_c_test_suite <- function() {
  assertthat::assert_that(over_estimate_third_party_test(),
                          msg="Failed third-party estimates") 
  assertthat::assert_that(over_estimate_rep_test(),
                          msg="Failed over-estimate Republican estimates") 
  assertthat::assert_that(over_estimate_dem_test(),
                          msg="Failed over-estimate Democratic estimates") 
}
estimate_c_test_suite() # stop everything if I broke this
```


```{r}
est_c_for_state <- function(state_abbrev) {
  
  tmp <- c(last(full_kalman(norm_approx(filter(polls_2016, state==state_abbrev), trump, clinton), election_date_2016)$prob))
  estimates <- tibble(democrat = tmp[[1]],
                      republican = tmp[[2]],
                      third_party = tmp[[3]],
                      undecided = tmp[[4]])
  
  results <- filter(results_2016, state == expand_abbrev[[state_abbrev]]) %>%
    dplyr::select(-state) %>%
    mutate(undecided=0)
  
  # print(estimates)
  # print(results)
  
  estimate_c(results, estimates)
}
```
We then take our results, transform the multivariate normal distribution into a univariate normal distribution using the estimates we just considered:

```{r}
prob_dem_win <- function(results, state_abbrev=NA, min_margin_of_victory=2, election_date=NA) {
  mu <- last(results$x)
  sigma <- last(results$p_mat)
  last_date <- last(results$date)
  
  walk_distance <- ifelse(is.na(election_date),
                          1,
                          sqrt(max(1, as.numeric(election_date-last_date))))
  
  if (is.na(state_abbrev)) {
    c <- matrix(data = c(1,-1,-1/4,-1/4), ncol=1)
    # c <- matrix(data = c(1,-1,0.345*(-0.1790679),-0.1790679), ncol=1)
  } else {
    c <- est_c_for_state(state_abbrev)
  }
  
  s2 <- t(c) %*% (sigma*walk_distance) %*% c
  
  probability <- pnorm(min_margin_of_victory, t(c) %*% mu, sd=sqrt(abs(s2)), lower.tail=F)
  # print(sprintf("S2 = %f, mu = %f, prob = %f", s2, t(c) %*% mu, probability))
  return(probability)
}
```

```{r}
mean_sd <- function(results) {
  s <- 0
  for(x in results$x) {
    s <- s + x
  }
  mu <- s/length(results$x)
  s2 <- 0
  for(x in results$x) {
    s2 <- s2 + (x - mu)**2
  }
  list(var = s2/length(results$x),
       mean = mu)
}
```

Iterate through the states with polls available "this far out" from election day back in 2016, and save the probability the Democrat would win for each state.

```{r}
run_2016 <- function(end_dt = today() - years(4)) {
  results <- list();
  for (state_name in unique(polls_2016$state)) {
    if (state_name != "--") {
      state_polls <- polls_2016 %>%
        filter(state == state_name,
               end.date <= end_dt)
      if (nrow(state_polls) > 1) {
        # print(state_name)
        polls <- norm_approx(polls_2016 %>% 
                               filter(state == state_name,
                                      end.date <= end_dt),
                             trump, clinton);
        results[[state_name]] <- prob_dem_win(full_kalman(polls, election_date_2016), election_date=election_date_2016)
      }
    }
  }
  results;
}
```

Now we save the electoral delegates to the most likely winner for each state.

```{r}
expected_ev <- function(forecast) {
  acc <- list(dem = 0, rep = 0);
  for (state in names(forecast)) {
    if (is.na(forecast[[state]])) {
      
    } else {
      party <- ifelse(forecast[[state]] < 0.5, "rep", "dem")
      acc[[party]] <- acc[[party]] + electoral_delegates.abbrev[[state]];
    }
  }
  acc;
}
```

# Projecting for 2020

Now, we can move forward and start projecting for 2020. First a helper function to download the latest polls.

```{r}
polls_2020_url <- "https://docs.google.com/spreadsheets/d/e/2PACX-1vQ56fySJKLL18Lipu1_i3ID9JE06voJEz2EXm6JW4Vh11zmndyTwejMavuNntzIWLY0RyhA1UsVEen0/pub?gid=0&single=true&output=csv"


download_latest_2020_polls <- function() {
  if(file.exists("../data/polling/all_polls_2020.csv")) {
    file.rename("../data/polling/all_polls_2020.csv",
                paste0("../data/polling/all_polls_2020.",
                       as.numeric(now()),
                       ".csv"))
  }
  download.file(polls_2020_url, 
              "../data/polling/all_polls_2020.csv", method = "auto")
  # if the file has data (i.e., we succeeded in downloading it)
  # then rename it to "all_polls_2020.csv"
}

download_latest_2020_polls()
```

```{r}
polls_2020_raw <- read_csv("../data/polling/all_polls_2020.csv",
                       col_types = cols(
                           .default = col_character(),
  start.date = col_date(format = "%m/%d/%Y"),
  end.date = col_date(format = "%m/%d/%Y"),
  entry.date.time..et. = col_datetime(format = "%m/%d/%Y %H:%M:%S"),
  number.of.observations = col_double(),
  trump = col_double(),
  biden = col_double(),
  other = col_double(),
  undecided = col_double()
                       )) %>%
  mutate(
  pollster = replace(pollster, "ABC News/Washington Post" == pollster,"ABC News/The Washington Post"),
  pollster = replace(pollster, "Gonzales Research & Media Services" == pollster,"Gonzales Research & Marketing Strategies Inc."),
  pollster = replace(pollster, "Mason-Dixon Polling & Research Inc." == pollster,"Mason-Dixon Polling & Strategy"),
  pollster = replace(pollster, "Meeting Street Insights" == pollster, "Meeting Street Research"),
 pollster = replace(pollster, "Monmouth" == pollster,"Monmouth University"),
  pollster = replace(pollster, "NBC News/Wall Street Journal" == pollster, "NBC News/The Wall Street Journal"),
  pollster = replace(pollster, "NYT/Siena College" == pollster, "Siena College/The New York Times Upshot"),
  pollster = replace(pollster, "Pulse Opinion Research" == pollster,"Rasmussen Reports/Pulse Opinion Research")) %>%
  as.data.frame
```

We need to normalize the pollster names...because no one agrees on how to do this, apparently. We should adjust for house effects, the simplest way to do this is to treat (for example) a "D +4.1" as a shift vector `c(4.1, -4.1, 0, 0)`. Hence we want to subtract this out from the polls produced by the firm with this House Effect. At least, that's the simplest first-order solution.

```{r}
download_latest_pollster_ratings <- function() {
  download.file("https://raw.githubusercontent.com/fivethirtyeight/data/master/pollster-ratings/pollster-ratings.csv", 
              "../data/polling/ratings_2020.csv", method = "auto")
}
download_latest_pollster_ratings()
```

We will setup a function to take a house effect string, then parse it into a 4-component vector.

```{r}
parse_house_effect <- function(house_effect) {
  if (is.na(house_effect)) { return(matrix(c(0,0,0,0), ncol=1)); }
  
  if ("D" == substring(house_effect, 1, 1)) {
    delta <- parse_number(house_effect)
    return(matrix(c(-delta, delta, 0, 0), ncol=1));
  } else {
    delta <- parse_number(house_effect)
    return(matrix(c(delta, -delta, 0, 0), ncol=1));
  }
}
```

```{r}
pollster_ratings_2020 <- read_csv("../data/polling/ratings_2020.csv")
```

So now we can add in the House effect to the polling data:
```{r}
polls_2020 <- polls_2020_raw %>%
  filter(!is.na(end.date),
         !is.na(start.date)) %>%
  left_join(pollster_ratings_2020 %>%
              dplyr::select(Pollster,
                            `Mean-Reverted Bias`,
                            `Predictive    Plus-Minus`,
                            `Banned by 538`) %>%
              rename(pollster = Pollster,
                     banned_538 = `Banned by 538`,
                     house_effect = `Mean-Reverted Bias`,
                     predictive = `Predictive    Plus-Minus`),
            by="pollster") %>%
  mutate(state = ifelse("FLA" == state, "FL", state),
         banned = (banned_538 == "yes")) %>%
  filter(banned_538 != T)

year(polls_2020$end.date) <- 2020
year(polls_2020$start.date) <- 2020
```

```{r}
polls_2016 <- polls_2016_raw %>%
  left_join(pollster_ratings_2020 %>%
              dplyr::select(Pollster,
                            `Mean-Reverted Bias`,
                            `Predictive    Plus-Minus`,
                            `Banned by 538`) %>%
              rename(pollster = Pollster,
                     banned_538 = `Banned by 538`,
                     house_effect = `Mean-Reverted Bias`,
                     predictive = `Predictive    Plus-Minus`),
            by="pollster") %>%
  mutate(state = ifelse("FLA" == state, "FL", state),
         banned = (banned_538 == "yes"),
         house_effect = ifelse(is.na(house_effect), 0, house_effect))
```

Lets examine the frequencies

```{r}
kable(polls_2020 %>% filter(#entry.date.time..et. >= today() - weeks(2)
                            end.date >= today() - weeks(1)) %>% group_by(state) %>% summarize(count = n()) %>% arrange(-count))
```


A helper function to automatically run the Kalman filter on a state:

```{r}
filter_state <- function(state_name, polls, election_date, dem=biden, rep=trump) {
  democrat <- enquo(dem)
  republican <- enquo(rep)
  full_kalman(norm_approx(polls %>%
                            filter(state == state_name),
                          dem=(!!democrat),
                          rep=(!!republican)),
              election_date)
}
```


We can now automate running the Kalman filter for the states:


```{r}
election_date_2020 <- ymd("2020-11-03")

run_2020 <- function(end_date = today(), is_nowcast = F, remove_banned=F) {
  results <- list();
  for (state_name in unique(polls_2020$state)) {
    state_polls <- polls_2020 %>%
      filter(state == state_name,
             banned %in% ifelse(remove_banned, c(F), c(F,T)),
             end.date <= end_date)
    if ((state_name != "--") && (nrow(state_polls) > 0)) {
      print(state_name)
      
      
      if (remove_banned) {
        polls <<- state_polls %>%
          filter(banned == F) %>%
          norm_approx(trump, biden)
      } else {
        polls <<- norm_approx(state_polls, trump, biden);
      }
      if (nrow(polls) > 0) {
        # print(sprintf("%s probability %f", state_name, prob_dem_win(full_kalman(polls, election_date_2020), state_name)), election_date=election_date_2020)
        if (!is_nowcast) {
          dt <- election_date_2020
        }
        
        results[[state_name]] <- prob_dem_win(full_kalman(polls, election_date_2020),
                                              state_abbrev=state_name,
                                              election_date=dt)
      }
    }
  }
  results;
}
```

For the sake of curiosity, let's create a data frame whose rows are states, and columns are the percent support for the Democratic candidate, Republican candidate, third parties, and the undecided voters.

```{r}
percentage_support <- function(polls, election_date, dem, rep, end_date = today(), is_nowcast = F, remove_banned=F, all_entries=F) {
  democrat <- enquo(dem)
  republican <- enquo(rep)
  results <- data.frame(state = c(),
                        democrat = c(),
                        republican = c(),
                        other = c(),
                        undecided = c(),
                        date = c());
  for (state_name in unique(polls$state)) {
    if (state_name != "--") {
      print(state_name)
      
      
      if (remove_banned) {
        state_polls <- polls %>%
          filter(state == state_name,
                 banned == F,
                 end.date <= end_date)
      } else {
        state_polls <- polls %>%
          filter(state == state_name,
                 end.date <= end_date)
      }
      
      
      if (nrow(state_polls) > 0) {
        normed_polls <- norm_approx(state_polls,
                                    !!republican, !!democrat);
        filtered_polls <- full_kalman(normed_polls, election_date)
        N <- length(filtered_polls$prob)
        
        for (i in ifelse(all_entries, 1, N):N) {
          probabilities <- filtered_polls$prob[[i]]
          results <- rbind(results,
                           tibble(state = state_name,
                                  democrat = probabilities[[1]],
                                  republican = probabilities[[2]],
                                  other = probabilities[[3]],
                                  undecided = probabilities[[4]],
                                  date = filtered_polls$date[[i]]))
        }
      }
    }
  }
  results;
}

prob_2020 <- function(end_date = today(), is_nowcast = F, remove_banned=F, all_entries=F) {
  percentage_support(polls_2020, election_date_2020, biden, trump, end_date, is_nowcast, remove_banned, all_entries)
}
```

```{r}
prob_2016 <- function(end_date = today() - years(4), is_nowcast = F, remove_banned=F, all_entries=F) {
  percentage_support(polls_2016, election_date_2016, clinton, trump, end_date, is_nowcast, remove_banned, all_entries)
}
```

We compute the Kalman filtered polling results:

```{r}
p20 <- prob_2020()
p20_all <- prob_2020(all_entries=T)
p16_all <- prob_2016(all_entries=T)
```

We can list the "competitive states" (where the margin of victory is less than the undecideds)

```{r}
# p20 <- prob_2020()
kable(arrange(p20 %>% mutate(biden = democrat - republican), -biden) %>% filter(abs(biden) <= undecided))
```

```{r}
# p20_all <- prob_2020(all_entries=T)

chart_support_over_time <- function(probabilities_df, state_abbrev) {
  ggplot(data=filter(probabilities_df, state==state_abbrev) %>%
           gather("party", "percent", republican, democrat, undecided, other) %>%
           mutate(party = factor(party, c("republican", "democrat", "undecided", "other")),
                  percent = 100*percent,
                  days = floor_date(today(), unit="years") + (date - floor_date(date, unit="years"))),
         aes(x=days, y=percent, group=party)) +
    geom_line(aes(color=party)) + 
    geom_point(aes(color=party)) +scale_color_manual(values=c("red", "#0015BC", "#daa520", "purple"))
}
```

```{r}
chart_support_over_time(p20_all, "MI")
```

```{r}
chart_support_over_time(filter(p16_all, date >= ymd('2016-03-01'),
                               date <= today()), "MI")
```

I want to see how Biden's support has changed, compared to Clinton's support (and similarly for Trump in '16 to Trump in '20). Let's write a modified helper function:


```{r}
# p20_all <- prob_2020(all_entries=T)

chart_support_comparison_over_time <- function(prob_16, prob_20, state_abbrev, rnc_end_date = ymd('2020/08/27')) {
  probabilities_df_16 <<- filter(prob_16, state==state_abbrev) %>%
    rename(trump_16=republican,
           clinton_16=democrat,
           undecided_16=undecided,
           other_16=other) %>%
    gather("party", "percent", trump_16, clinton_16, undecided_16, other_16) %>%
    mutate(party = factor(party, c("trump_16", "clinton_16", "undecided_16", "other_16", "trump_20", "biden_20", "undecided_20", "other_20")),
           percent = 100*percent,
           days = floor_date(today(), unit="years") + (date - floor_date(date, unit="years"))) %>%
    dplyr::select(state, party, percent, days)
  
  probabilities_df_20 <<- filter(prob_20, state==state_abbrev) %>%
    rename(trump_20=republican,
           biden_20=democrat,
           undecided_20=undecided,
           other_20=other) %>%
    gather("party", "percent", trump_20, biden_20, undecided_20, other_20) %>%
    mutate(party = factor(party, c("trump_16", "clinton_16", "undecided_16", "other_16", "trump_20", "biden_20", "undecided_20", "other_20")),
           percent = 100*percent,
           days = floor_date(today(), unit="years") + (date - floor_date(date, unit="years"))) %>%
    dplyr::select(state, party, percent, days)
  
  probabilities_df <<- rbind(probabilities_df_16, probabilities_df_20)
  
  # return(probabilities_df)
  
  ggplot(data=probabilities_df,
         aes(x=days, y=percent, group=party)) +
    geom_vline(xintercept=as.numeric(rnc_end_date), linetype=4) +
    annotate("text",
             x=(rnc_end_date), y=25,
             label="\nRNC ends",
             angle=90, size=3.25) +
    geom_line(aes(color=party)) + 
    geom_point(aes(color=party)) + # "#FFC0CB"
    scale_color_manual(values=c("#FFC0CB", "lightblue", "lightgoldenrod", "plum2", # "hotpink",
                                "red", "#0015BC", "#daa520", "purple")) +
    scale_x_date(date_labels = "%b",
                 date_breaks = "1 months") +
  labs(title = paste0("Comparing ", state_abbrev," Polls from 2016 to 2020"),
       subtitle = paste0("State-level Presidential polls, Kalman filtered, ",
                         format(today(), "%b %d, %Y")),
       caption = "https://political-arithmetic.blogspot.com")
}
```

```{r}
kable(polls_2020 %>% filter(#entry.date.time..et. >= today() - weeks(2)
                            start.date + 0.5*(end.date - start.date) >= today() - days(10)) %>% group_by(state) %>% summarize(count = n()) %>% arrange(-count))
```

Now we save the plot to a file.

```{r}
save_comparison_plot <- function(prob_16, prob_20, state_abbrev, date_run = today()) {
  plot <- chart_support_comparison_over_time(prob_16, prob_20 %>% filter(date <= date_run), state_abbrev)
  outfile <- sprintf("../img/2020-plots/comparison-%s-%s.png", state_abbrev, date_run)
  ggsave(outfile, plot=plot, height=4, dpi=500)
  plot
}
```

```{r}
# tail(p20_all %>% filter(state == "GA"))
chart_support_comparison_over_time(p16_all, p20_all, "MN")
```

```{r}
# tail(p20_all %>% filter(state == "FL"))
save_comparison_plot(p16_all, p20_all, "FL")
```

```{r}
save_comparison_plot(p16_all, p20_all, "MI")
```

```{r}
save_comparison_plot(p16_all, p20_all, "GA")
```

```{r}
save_comparison_plot(p16_all, p20_all, "WI")
```


```{r}
# p20_all <- prob_2020(all_entries=T)
# p16_all <- prob_2016(all_entries=T)

predict_for_election <- function(state_abbrev) {
  list(
    biden_2020 = predict(lm(democrat ~ dt, 
                            filter(p20_all,
                                   date >= ymd('2016-03-01'),
                                   state==state_abbrev) %>%
                              mutate(dt = as.numeric(date - ymd('2020-01-01')))),
                         newdata = tibble(dt = as.numeric(ymd('2020-11-03')-ymd('2020-01-01')))),
    trump_2020 = predict(lm(republican ~ dt,
                            filter(p20_all,
                                   date >= ymd('2016-03-01'),
                                   state == state_abbrev) %>%
                              mutate(dt = as.numeric(date - ymd('2020-01-01')))),
                         newdata = tibble(dt = as.numeric(ymd('2020-11-03')-ymd('2020-01-01')))),
    clinton_2016 = predict(lm(democrat ~ dt,
                              filter(p16_all,
                                     date >= ymd('2016-03-01'),
                                     date <= today() - years(4),
                                     state == state_abbrev) %>%
                                mutate(dt = as.numeric(date - ymd('2020-01-01')))),
                           newdata = tibble(dt = as.numeric(ymd('2016-11-08')-ymd('2016-01-01')))),
    trump_2016 = predict(lm(republican ~ dt,
                            filter(p16_all,
                                   date >= ymd('2016-03-01'),
                                   date <= today() - years(4),
                                   state == state_abbrev) %>%
                              mutate(dt = as.numeric(date - ymd('2020-01-01')))),
                         newdata = tibble(dt = as.numeric(ymd('2016-11-08')-ymd('2016-01-01'))))
  )
}
```



```{r}
filter(polls_2020, state=="MI") %>% dplyr::select(start.date, end.date) %>% group_by(start.date, end.date) %>% summarize(count = n())
```

## Words of Estimative Probability

The ratings may be estimated qualitatively as "Safe", "Likely", "Lean", "Toss up" (and sometimes "Tilt"). Based off of G. Elliott Morris' [article](http://centerforpolitics.org/crystalball/articles/two-ways-of-thinking-about-election-predictions-and-what-they-tell-us-about-2018/):

> Each of these categories also has a corresponding Democratic probability of victory for the seats placed within. In Lean Democratic seats, Democrats win the elections 78% of the time; Lean Republican: 18%; Likely D: 95%; Likely R: 3%; Safe D: 99%; Safe R 0%, and Toss-up districts: 59%. These values are plotted on the right of the preceding figure, with the size of each point showing the number of seats earning that designation over the years.

A more [thorough review](https://cookpolitical.com/accuracy) of the Cook political report finds similar assessments, although their "Lean <party>" corresponds to an 85% probability the predicted candidate would win. Curiously, this seems to correspond to approximately to the logit scale of +2 for lean, +3 for likely, and +5 for solid. This would suggest a tilt should have +1 on the logit scale, corresponding to a probability 73% favoring the predicted candidate.


```{r}
inv_logit <- function(x) 1/(1+exp(-x))
ratings <- list(
  lean_dem = 0.8,
  lean_rep = 0.2,
  likely_dem = 0.95,
  likely_rep = 0.05,
  safe_dem = 0.99,
  safe_rep = 0.01,
  toss_up = 0.5,
  # interpolated using geometric mean
  tilt_dem = 0.65,
  tilt_rep = 0.35
)
ratings <- list(
  lean_dem = inv_logit(3/4),
  lean_rep = inv_logit(-3/4),
  likely_dem = inv_logit(4),
  likely_rep = inv_logit(-4),
  safe_dem = inv_logit(5),
  safe_rep = inv_logit(-5),
  toss_up = inv_logit(0),
  # interpolated using geometric mean
  tilt_dem = inv_logit(1/2),
  tilt_rep = inv_logit(-1/2)
)

qualitative_label <- function(prob_dem_win) {
  # original ratings estimate
  # ratings <- list(lean_dem = 0.78,
  #                 lean_rep = 0.18,
  #                 likely_dem = 0.95,
  #                 likely_rep = 0.03,
  #                 safe_dem = 0.99,
  #                 safe_rep = 0,
  #                 toss_up = 0.59)

  delta <- 1
  label <- NA
  for (rating in names(ratings)) {
    tmp <- abs(prob_dem_win - ratings[[rating]])
    if (tmp < delta) {
      delta <- tmp
      label <- rating
    }
  }
  label  
}
```

For the sake of debugging how I'm assigning labels, I have this helper function to print out what probabilities correspond to what labels. If I were smarter, I would create something visual, or just have the endpoints computed.

```{r}
test_labels <- function(n=20) {
  result <- c()
  for (i in (0:n)) {
    result <- c(result,  qualitative_label(i/n))
  }
  result
  data.frame(label = result,
             probability = (0:n)/n)
}
```

We can then print out the qualitative estimates for an election as:

```{r}
dump_forecast <- function(prediction) {
  states <- c()
  labels <- c()
  for (state in sort(names(prediction))) {
    if (!is.na(prediction[[state]])) {
      states <- c(states, state)
      labels <- c(labels, qualitative_label(prediction[[state]]))
    }
  }
  data.frame(state_abbv = states,
             prediction = labels)
}
```

```{r}
plot_prediction <- function(df) {
  spatial_data <- left_join(get_urbn_map(map = "states", sf = TRUE),
                            df,
                            by = "state_abbv")
  spatial_data
    p <- ggplot() +
    geom_sf(
      sdf1,
      mapping = aes(fill = probability),
      color = "#ffffff",
      size = 0.25) +
    labs(fill = "Homeownership rate")
  return(spatial_data)
}

```

We can plot the probability that Biden will win the election in a state:

```{r}
plot_biden_win <- function(state_name, is_nowcast = F) {
  forecast <- full_kalman(norm_approx(filter(polls_2020, state==state_name), trump, biden), election_date_2020)
  c <- matrix(data=est_c_for_state(state_name),ncol=1)
  mu <- t(c) %*% last(forecast$x)
  latest_poll_date <- last(sort(forecast$date))
  sigma <- sqrt(abs((t(c) %*% (last(forecast$p_mat) * ifelse(is_nowcast, 1, sqrt(max(1, as.numeric(election_date_2020 - latest_poll_date))))) %*% c)))
  #print(c) 
  #print(last(forecast$x))
  #print(last(forecast$p_mat))
  #print(sprintf("mu = %f, sigma = %f", mu, sigma))
  delta <- min(0.01, (6*sigma)/100)

  pic <- ggdistribution(dnorm, seq(2, mu + 4*sigma, by=delta), mean=mu, sd=sigma, fill='blue')
  ggdistribution(dnorm, seq(-4*sigma + mu, 2, by=delta), mean=mu, sd=sigma, fill='red', p = pic) +
    labs(title = sprintf("Probability of Biden winning %s", state_name),
       subtitle = sprintf("Kalman filter of state-level polls as of %s (prob %.3f%%)",
                         today(),
                         prob_dem_win(forecast, state_name, election_date=election_date_2020)*100),
       caption = "https://political-arithmetic.blogspot.com")
}
```

I then work my way through the interesting states, saving them to files.

```{r}
print_plots <- function(states = c("AZ", "FL", "GA",
                                   "MI", "NV", "NC", "NH",
                                   "OH", "PA", "TX", "WI")) {
  for(state_name in states) {
    print(getwd())
    outfile <- sprintf("./img/2020-plots/%s-%s.png", state_name, today())
    print(outfile)
    p <- plot_biden_win(state_name)
    ggsave(outfile, plot=p)
  }
}
```


```{r}
use_house_effect <- F
date_run <- today() # ymd("2020-07-20")
f1 <- run_2020(date_run)
f0 <- dump_forecast(f1)
current_forecast <- rbind(f0,
             data.frame(state_abbv = names(electoral_delegates.abbrev),
                        prediction = rep("no_data", 51)) %>%
               filter(!(state_abbv %in% f0$state_abbv))) %>%
  mutate(prediction = factor(ifelse(is.na(prediction), "toss_up", prediction),
                             ordered=T,
                             levels=c("safe_rep",
                                      "likely_rep",
                                      "lean_rep",
                                      "tilt_rep",
                                      "toss_up",
                                      "tilt_dem",
                                      "lean_dem",
                                      "likely_dem",
                                      "safe_dem",
                                      "no_data"))) %>%
  arrange(state_abbv)

sdf1 <- inner_join(get_urbn_map(map = "states", sf=T),
                  current_forecast,
                  by="state_abbv")

color_code <- list("safe_rep" = "#800000",
                   "likely_rep" = "#ff0000",
                   "lean_rep" =  "#FF6347",
                   "tilt_rep" =  "#DB7093",
                   "toss_up" = "#daa520", # "#a9a9a9",
                   "tilt_dem" = "#ADD8E6",
                   "lean_dem" = "#00BFFF", 
                   "likely_dem" = "#1E90FF",
                   "safe_dem" =  "#00008B",
                   "no_data" = "#DCDCDC")

# ggthemes::colorblind_pal()(5)
plot <- ggplot() +
  geom_sf(sdf1, mapping = aes(fill = prediction),
          color = "#ffffff") +
  scale_fill_manual("Rating", values = unlist(color_code[sort(unique(current_forecast$prediction))])) +
  labs(fill = "Rating",
       title = "Kalman Filtered Poll Aggregator",
       subtitle = paste0("State-level Presidential polls applied ",
                         date_run),
       caption = "https://political-arithmetic.blogspot.com") +
  coord_sf(datum = NA)

outfile <- sprintf("../img/2020-plots/kalman-filter-%s-updated.png",date_run)
ggsave(outfile, plot=plot, height=4)
# print_plots() # use_house_effect_plot
# no_house_effect_plot
plot
```

```{r}
house_fx_plot <- plot
house_fx_plot
```

```{r}
source("../R/presidential_elections.R")
source("../R/states.R")
state_results <- load_obj(state_path)
state_results_2016 <- state_results %>%
  filter(year==2016, candidate=="Clinton, Hillary") %>%
  group_by(state_po) %>%
  summarize(results = sum(candidatevotes)/max(totalvotes)) %>%
  rename(state = state_po)
econ_predict_2016 <- read_rds("../data/elections/presidential/pct_clinton_full.rds")
naive_econ_predict_2016 <- read_rds("../data/elections/presidential/pct_clinton_april_1.rds")

df <- left_join(econ_predict_2016 %>%
                   filter(state != "--"),
                 state_results_2016,
                 by = "state") %>%
  mutate(term = results*log2(results/mean) + (1 - results)*log2((1 - results)/(1 - mean))) 

naive_deltas <- left_join(naive_econ_predict_2016 %>%
                   filter(state != "--", t==max(t)),
                 state_results_2016,
                 by = "state") %>%
  group_by(state) %>%
  summarize(diff = (mean - results))
deltas <- left_join(econ_predict_2016 %>%
                   filter(state != "--", t==max(t)),
                 state_results_2016,
                 by = "state") %>%
  group_by(state) %>%
  summarize(diff = (mean - results))
```

```{r}
df0 <- data.frame(states = as.character(df$state),
                  term = df$term)
df0$ev <- unlist(electoral_delegates.abbrev[df0$states])
df0$loss <- df0$term * df0$ev
#  mutate(ev = electoral_delegates.abbrev[states])
```

```{r}
us_est <- c(last(filter_state("--", polls_2016, election_date_2016, dem=clinton)$prob))
names(us_est) <- c('democrat', 'republican', 'third_party', 'undecided')
us_2016 <- c(65853514, 62984828,7830934,0)/136669276
#tx_2016 <- filter(results_2016, state=="Texas") %>%
#  dplyr::select(-state) %>%
#  mutate(undecided=0)
```

# Polling Error

The basic scheme for determining polling error is to construct the odds ratio for the poll (the proportion which supports the Democratic candidate to those supporting the Republican candidate), then divide by the odds ratio for the votes cast.

```{r}
state_results_2016 <- load_obj(state_path) %>%
  filter(year == 2016,
         candidate %in% c("Clinton, Hillary", "Trump, Donald J.")) %>%
  group_by(state_po, candidate) %>%
  summarize(cv = sum(candidatevotes),
            tv = max(totalvotes)) %>%
  rename(state = state_po) %>%
  mutate(party = ifelse(candidate == "Clinton, Hillary", "democrat", "republican")) %>%
  dplyr::select(-candidate) %>%
  spread(party, cv)
  
```

```{r}
odds_to_prob <- function(odds) {
  1 - (1/(1 + odds))
}
```

```{r}
polling_odds_16 <- polls_2016_raw %>%
  filter(state != "--") %>%
  inner_join(state_results_2016,
             by="state") %>%
  mutate(polling_odds = clinton/trump,
         voting_odds = democrat/republican,
         a = log(polling_odds/voting_odds))
```


```{r}
polling_odds_16 %>%
  dplyr::select(state, end.date, a, pollster) %>%
  filter(state %in% c("PA", "MI", "WI")) %>%
  arrange(a^2)
```