---
title: "Kalman Filtering the Polls"
author: "Alex Nelson"
date: "7/5/2020"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(lubridate)
library(truncnorm)
library(MASS)
library(kableExtra)
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Despite the name, a "Kalman filter" doesn't _filter_. It's more of a "guess-and-check"-er.

```{r}
polls_2016 <- read_csv("../data/polling/all_polls_2016.csv",
                       col_types = cols(
                           .default = col_character(),
  X1 = col_double(),
  start.date = col_date(format = ""),
  end.date = col_date(format = ""),
  entry.date.time..et. = col_datetime(format = ""),
  number.of.observations = col_double(),
  trump = col_double(),
  clinton = col_double(),
  other = col_double(),
  undecided = col_double(),
  question.iteration = col_double(),
  johnson = col_double(),
  mcmullin = col_double()
                       ))
```

## Simple filtering

So, the control vector would be the `entry.date.time..et.` date, because it's an external impulse.

```{r}
mi_2016 <- polls_2016 %>%
  filter(state == "MI") %>%
  arrange(entry.date.time..et.)

election_date_2016 <- ymd("2016-11-08")
```

We transform this into approximately normal variables

```{r}
mi_clinton <- mi_2016 %>%
  filter(partisan != "Pollster") %>%
  mutate(clinton_mu = (clinton/100),
         clinton_sd = qnorm(0.999)*sqrt(((clinton/100)*(1 - clinton/100) + (1 - (clinton+trump)/100)**2)/number.of.observations),
         t = 1+as.integer(end.date - min(mi_2016$end.date)))
```

```{r}
get_for_state <- function(state_str) {
  polls_2016 %>%
  filter(state == state_str) %>%
  arrange(entry.date.time..et.) %>%
  # filter(partisan != "Pollster") %>%
  mutate(clinton_mu = (clinton/100),
         sd = qnorm(0.999)*sqrt(((clinton/100)*(1 - clinton/100) + (1 - (clinton+trump)/100)**2)/number.of.observations),
         clinton_sd = sd,
         t = 1+as.integer(end.date - min(mi_2016$end.date)),
         precision = 1/clinton_sd**2)
}
```

```{r}
pool_polls <- function(df) {
  df %>%
    group_by(t) %>%
    summarize(clinton_pooled = weighted.mean(clinton/100, precision),
              trump_pooled = weighted.mean(trump/100, precision),
              trump = 100*trump_pooled,
              clinton = 100*clinton_pooled,
              sd = sqrt(1/sum(precision)),
              number.of.observations = sum(number.of.observations),
              end.date = first(end.date),
              number_of_polls = n()) %>%
    arrange(t)
}
```

This is a dummy Kalman filter running on a one-dimensional signal.

```{r}
kalman_filter <- function(df, mu, sd, election_dt = election_date_2016, h=1, f=1) {
  q = mean(df$sd)
  end_T <- as.integer(election_dt - min(df$end.date))
  x <- c(rnorm(1, 0.5, mean(df$sd)))
  p <- c(q)
  residual <- c()
  z <- rep(NA, end_T)
  r <- rep(NA, end_T)
  for (i in 1:nrow(df)) {
    z[df$t[i]] <- mu[i]
    r[df$t[i]] <- sd[i]
  }
  for (i in 1:end_T) {
    if (is.na(z[i])) {
      # random walk
      x <- c(x, rtruncnorm(1, a=0, b=1, mean=f*last(x), sd=q))
    } else {
      # predict
      x_pred <- f*last(x);
      p_pred <- f*last(p)*f + q;
      # correct
      y <- z[i] - h*x_pred;
      s <- h*p_pred*h + mean(r[1:i], na.rm=T);
      k <- p_pred*h/s;
      x <- c(x, x_pred + k*y)
      p <- c(p, (1 - k*h)*p_pred)
      residual <- c(residual, z[i] - h*last(x))
    }
  }
  list(x=x,
       p=p,
       residuals=residual)
}
```

```{r}
estimate_win <- function(df, n=100) {
  mean(replicate(last(kalman_filter(df, df$trump_pooled, df$sd)$x), n=n)  < replicate(last(kalman_filter(df, df$clinton_pooled, df$sd)$x), n=n))
}
```

# Refined Model

This time, lets use a multinomial distribution to describe a poll, then use the (multivariate) normal approximation which will be our `z[k]`. This will give us a normal variable `x[k][DEM]` for the estimated support for the Democratic candidate, and another `x[k][REP]` for the Republican candidate. Third party supporters become noise (more or less)...well, due to the quirkiness of the normal approximation to the multinomial, it's singular in nature (meaning: we can write `third_party = 1 - dem - rep` for the proportions), so we have an extra variable (`third_party`) that's not needed. (Just like real life.)


```{r}
multinomial_covariance_mat <- function(prob_vec, size) {
  mat <- matrix(data = prob_vec %*% t(prob_vec),
              nrow=length(prob_vec))*size;
  for (i in 1:length(prob_vec)) {
    mat[i,i] <- prob_vec[i]*(1 - prob_vec[i])
  }
  mat
}

norm_approx <- function(poll_df, rep, dem) {
  democrat <- enquo(dem)
  republican <- enquo(rep)
  poll_df %>%
    transmute(p_dem = !!democrat/100,
              p_rep = !!republican/100,
              p_third = 1 - p_dem - p_rep,
              precision = (qnorm(0.975)*0.5/sqrt(number.of.observations))**-2,
              start_date = start.date,
              end_date = end.date,
              published = as.Date(entry.date.time..et.),
              size = number.of.observations,
              state = state,
              population = population
              ) %>%
    group_by(state,end_date) %>%
    summarize(p_dem = max(0, weighted.mean(p_dem, precision)),
              p_rep = max(0, weighted.mean(p_rep, precision)),
              p_third = max(0, weighted.mean(p_third, precision)),
              precision = sum(precision),
              size = sum(size),
              mu_dem = size*p_dem,
              mu_rep = size*p_rep,
              mu_third = size*p_third,
              .groups = "drop")
}
```

Now, the `x_hat[k, k-1]` will need to be re-scaled to match the polling results, which is what happens with the `H[k]` matrix.

```{r}
make_h <- function(new_size, existing_size) {
  diag(new_size/existing_size, ncol=2, nrow=2)
}
```


```{r}
full_kalman <- function(df, election_date) {
  end_T <- as.integer(election_date - min(df$end_date))
  xs <- list()
  xs[[1]] <- (matrix(data=c(1600,1400,600), ncol=1))
  covariance_matrices <- list()
  covariance_matrices[[1]] <- 10000*matrix(data=c(50,-1/10,-1,
                                            -1/10,50,-1,
                                            -1,-1,5), nrow=3)
  residuals <- list()
  p <- list()
  
  
  for (i in 1:nrow(df)) {
    # estimate
    x_est <- last(xs)
    Q_mat <- diag((qnorm(0.975)**2)*0.5*df$size[i], 3);
    if (i > 1) {
      Q_mat <- Q_mat + multinomial_covariance_mat(c(df$p_dem[i],
                                          df$p_rep[i],
                                          df$p_third[i]),
                                        size  = df$size[i])
    }
    P_est <- last(covariance_matrices) + Q_mat
    
    # predict
    z <- matrix(data=c(df$mu_dem[i], df$mu_rep[i], df$mu_third[i]), ncol=1)
    
    h <- diag(1, nrow=3)
    if (i>1) {
      h <- diag(df$size[i]/max(df$size[i-1],sum(x_est)), 3)
      # print(df$size[i]/max(df$size[i-1],sum(x_est)))
    }
    
    y <- z - (h %*% x_est)
    
    R_mat <- multinomial_covariance_mat(c(df$p_dem[i],
                                          df$p_rep[i],
                                          df$p_third[i]),
                                        size  = df$size[i]) +
      diag((qnorm(0.975)**2)*0.5*df$size[i], 3);
    R_mat[3,c(1,2)] <- 0; R_mat[c(1,2),3] <- 0;
    s <- (h %*% P_est %*% t(h)) + R_mat
    kalman_gain <- P_est %*% t(h) %*% solve(s)
    
    x_hat <- matrix(data=(x_est + (kalman_gain %*% y)), ncol=1)
    xs[[length(xs)+1]] <- x_hat
    p[[length(p) + 1]] <- x_hat/df$size[i]
    
    P_mat <- (diag(1, 3) - (kalman_gain %*% h)) %*% P_est %*% t((diag(1, 3) - (kalman_gain %*% h)))
    P_mat <- P_mat + (kalman_gain %*% R_mat %*% t(kalman_gain))
    covariance_matrices[[length(covariance_matrices)+1]] <- P_mat
    
    residuals[[length(residuals)+1]] <- (z - (h %*% x_hat))
  }
  
  list(x = xs,
       p_mat = covariance_matrices,
       prob = p,
       residuals = residuals)
}
```

We then can transform the result into a probability of winning

```{r}
prob_dem_win <- function(results) {
  mu <- last(results$x)
  sigma <- last(results$p_mat)
  c <- matrix(data = c(1,-1,-1/2), ncol=1)
  s2 <- t(c) %*% sigma %*% c
  pnorm(0.5, t(c) %*% mu, sd=sqrt(abs(s2)), lower.tail=F)
}
```

```{r}
mean_sd <- function(results) {
  s <- 0
  for(x in results$x) {
    s <- s + x
  }
  mu <- s/length(results$x)
  s2 <- 0
  for(x in results$x) {
    s2 <- s2 + (x - mu)**2
  }
  list(var = s2/length(results$x),
       mean = mu)
}
```

Iterate through the states with polls available "this far out" from election day back in 2016, and save the probability the Democrat would win for each state.

```{r}
run_2016 <- function(end_date = today() - years(4)) {
  results <- list();
  for (state in unique(polls_2016$state)) {
    if (state != "--") {
      polls <- norm_approx2(get_for_state(state) %>% 
                              filter(end.date <= end_date),
                            dem=clinton, rep=trump);
      if (nrow(polls) > 1) {
        results[[state]] <- prob_dem_win(full_kalman(polls, election_date))
      }
    }
  }
  results;
}
```

Now we save the electoral delegates to the most likely winner for each state.

```{r}
expected_ev <- function(forecast) {
  acc <- list(dem = 0, rep = 0);
  for (state in names(forecast)) {
    if (is.na(forecast[[state]])) {
      
    } else {
      party <- ifelse(forecast[[state]] < 0.5, "rep", "dem")
      acc[[party]] <- acc[[party]] + electoral_delegates.abbrev[[state]];
    }
  }
  acc;
}
```

# Projecting for 2020

Now, we can move forward and start projecting for 2020.

```{r}
polls_2020 <- read_csv("../data/polling/all_polls_2020.csv",
                       col_types = cols(
                           .default = col_character(),
  start.date = col_date(format = "%m/%d/%Y"),
  end.date = col_date(format = "%m/%d/%Y"),
  entry.date.time..et. = col_datetime(format = "%m/%d/%Y %H:%M:%S"),
  number.of.observations = col_double(),
  trump = col_double(),
  biden = col_double(),
  other = col_double(),
  undecided = col_double()
                       ))
```

Lets examine the frequencies

```{r}
kable(polls_2020 %>% group_by(state) %>% summarize(count = n()) %>% arrange(-count))
```



We can now automate running the Kalman filter for the states:


```{r}
election_date_2020 <- ymd("2020-11-03")
run_2020 <- function() {
  results <- list();
  for (state_name in unique(polls_2020$state)) {
    if (state_name != "--") {
      polls <- norm_approx2(polls_2020 %>% filter(state == state_name),
                            dem=biden, rep=trump);
      if (nrow(polls) > 1) {
        results[[state_name]] <- prob_dem_win(full_kalman(polls, election_date_2020))
      }
    }
  }
  results;
}
```

## Words of Estimative Probability

The ratings may be estimated qualitatively as "Safe", "Likely", "Lean", "Toss up" (and sometimes "Tilt"). Based off of G. Elliott Morris' [article](http://centerforpolitics.org/crystalball/articles/two-ways-of-thinking-about-election-predictions-and-what-they-tell-us-about-2018/):

> Each of these categories also has a corresponding Democratic probability of victory for the seats placed within. In Lean Democratic seats, Democrats win the elections 78% of the time; Lean Republican: 18%; Likely D: 95%; Likely R: 3%; Safe D: 99%; Safe R 0%, and Toss-up districts: 59%. These values are plotted on the right of the preceding figure, with the size of each point showing the number of seats earning that designation over the years.



```{r}
qualitative_label <- function(prob_dem_win) {
  # original ratings estimate
  # ratings <- list(lean_dem = 0.78,
  #                 lean_rep = 0.18,
  #                 likely_dem = 0.95,
  #                 likely_rep = 0.03,
  #                 safe_dem = 0.99,
  #                 safe_rep = 0,
  #                 toss_up = 0.59)
  ratings <- list(lean_dem = 0.8,
                  lean_rep = 0.2,
                  likely_dem = 0.95,
                  likely_rep = 0.05,
                  safe_dem = 0.99,
                  safe_rep = 0.01,
                  toss_up = 0.5,
                  # interpolated using geometric mean
                  tilt_dem = 0.65,
                  tilt_rep = 0.35)
  delta <- 1
  label <- NA
  for (rating in names(ratings)) {
    tmp <- abs(prob_dem_win - ratings[[rating]])
    if (tmp < delta) {
      delta <- tmp
      label <- rating
    }
  }
  label  
}
```

We can then print out the qualitative estimates for an election as:

```{r}
dump_forecast <- function(prediction) {
  for (state in sort(names(prediction))) {
    print(paste0(state, " = ", qualitative_label(prediction[[state]])))
  }
}
```